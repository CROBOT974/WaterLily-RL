{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306c8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:01.082213Z",
     "start_time": "2025-08-28T10:02:49.597887Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "\n",
    "os.environ[\"JULIA_NUM_THREADS\"] = \"8\"\n",
    "from julia import Julia\n",
    "jl = Julia(compiled_modules=False)\n",
    "\n",
    "from julia import Main\n",
    "print(Main.eval(\"Threads.nthreads()\"))\n",
    "from model.VIV_gym import JuliaEnv\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Lib支持\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, CallbackList\n",
    "\n",
    "# dir videos\n",
    "video_folder = \"./videos/\"\n",
    "os.makedirs(video_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fe78b430b301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:44.764763Z",
     "start_time": "2025-08-28T10:03:44.757683Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "反馈reward和建立checkpoint\n",
    "\n",
    "\"\"\"\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = None\n",
    "        self.episode_steps = []          # 存每个 episode 的 step 数\n",
    "        self.current_steps = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.current_rewards = np.zeros(self.training_env.num_envs)\n",
    "        self.current_steps = np.zeros(self.training_env.num_envs, dtype=int)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        self.current_rewards += rewards\n",
    "        self.current_steps += 1   # 每个 step 累加\n",
    "\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                self.episode_rewards.append(self.current_rewards[i])\n",
    "                self.episode_steps.append(self.current_steps[i])  # 记录步数\n",
    "\n",
    "                print(f\"Episode finished after {self.current_steps[i]} steps\")\n",
    "                print(f\"Episode reward: {self.current_rewards[i]:.2f}\")\n",
    "                # reset\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.current_steps[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq= 1000,\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7553a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:45.729164Z",
     "start_time": "2025-08-28T10:03:45.717647Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "训练用参数(VIV)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 16\n",
    "def pos_generator():\n",
    "    return [0.0, np.random.uniform(- diameter/6, diameter/6)]\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,\n",
    "    \"action_scale\": 0.5,\n",
    "    \"size\": [10, 8],\n",
    "    \"location\": [3, 4]\n",
    "}\n",
    "#variable parameters\n",
    "variables = {\n",
    "    \"position\":[0.0, diameter/6],\n",
    "    \"velocity\":[0.0, 0.0]\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,\n",
    "    \"observation\":3\n",
    "}\n",
    "\n",
    "from model.VIV_gym import VIVEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "训练用参数(FOIL)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 16\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,\n",
    "    \"F_scale\": 10,\n",
    "    \"size\": [8, 6],\n",
    "    \"nose\": [1, 4],\n",
    "    \"rot_center\":[0.25,0]\n",
    "}\n",
    "#variable parameters\n",
    "variables = {\n",
    "    \"position\":[0.0, 0.0],\n",
    "    \"velocity\":[0.0, 0.0],\n",
    "    \"theta\":0.05 * np.pi,\n",
    "    \"rot_vel\": 0.0,\n",
    "    \"rot_acc\": 0.0\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,\n",
    "    \"observation\":5\n",
    "}\n",
    "\n",
    "from model.VIV_gym import FoilEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "训练用参数(Drag)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 48\n",
    "\n",
    "def ksi_generator():\n",
    "    return np.random.uniform(3.0, 4.0)\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,\n",
    "    \"F_scale\": 8,\n",
    "    \"L_ratio\": 0.15,\n",
    "    \"L_gap\":0.05,\n",
    "    \"location\": [2, 0],\n",
    "    \"size\": [6, 2]\n",
    "}\n",
    "# variable parameters\n",
    "variables = {\n",
    "    \"ksi\": ksi_generator\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,\n",
    "    \"observation\":2\n",
    "}\n",
    "\n",
    "from model.VIV_gym import DragEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbdcbcd7d8e0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:06:25.845138Z",
     "start_time": "2025-08-28T10:03:47.256889Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "单线程环境建立，训练，保持\n",
    "\n",
    "\"\"\"\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "model.learn(total_timesteps=150_000, callback = callback)\n",
    "model.save(\"SAC_model\")\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a89abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "单线程环境建立，训练，保持(注册表)\n",
    "\n",
    "\"\"\"\n",
    "env = DummyVecEnv([lambda: gym.make(\"VIV-v0\")])\n",
    "\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "model.learn(total_timesteps=2_000, callback = callback)\n",
    "model.save(\"PPO_model\")\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "多线程环境建立\n",
    "\n",
    "\"\"\"\n",
    "def make_env(rank: int, seed: int = 0):\n",
    "    def _init():\n",
    "        env = JuliaVIVEnv(render_mode=None, max_episode_steps=200, verbose=1)\n",
    "        env.reset(seed=seed+rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "num_envs = 4\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_envs)])\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "\n",
    "model.learn(total_timesteps=20_000, callback = callback)\n",
    "model.save(\"ppo_model\")\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf25679942131",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(reward_callback.episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e00a8abd73c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "加载checkpoint并继续训练\n",
    "\n",
    "\"\"\"\n",
    "def make_env(rank: int, seed: int = 0):\n",
    "    def _init():\n",
    "        env = JuliaDragEnv(render_mode=None, max_episode_steps=30, verbose=1)\n",
    "        env.reset(seed=seed+rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "num_envs = 4\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_envs)])\n",
    "\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "\n",
    "model = PPO.load(\"./checkpoints/ppo_model_40000_steps\", env=env, device='cpu')\n",
    "model.learn(total_timesteps=20_000, callback = callback)\n",
    "rewards_ex = np.array(reward_callback.episode_rewards)\n",
    "rewards = [rewards, rewards_ex]\n",
    "model.save(\"./model_stage/ppo_model_40k\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930ccd7507654ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T03:35:29.970382Z",
     "start_time": "2025-08-11T03:35:23.541861Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "绘图功能\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 参数：滑动窗口大小\n",
    "window = 10\n",
    "\n",
    "def plot_rewards(rewards, window=100):\n",
    "    episode = np.arange(len(rewards))\n",
    "\n",
    "    # 计算滑动均值和标准差\n",
    "    def moving_avg(x, w):\n",
    "        return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    mean = moving_avg(rewards, window)\n",
    "    std = np.array([\n",
    "        np.std(rewards[max(0, i - window + 1):i + 1])\n",
    "        for i in range(window - 1, len(rewards))\n",
    "    ])\n",
    "\n",
    "    # 对应 x 轴\n",
    "    x = np.arange(window - 1, len(rewards))\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, mean, label='Mean Reward')\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3, label='±1 Std Dev')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward over Training\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards,window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8e6b296ccfe1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-11T03:35:38.095121Z"
    }
   },
   "outputs": [],
   "source": [
    "#训练结束后出gif\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from model.VIV_gym import JuliaEnv\n",
    "from gif import create_GIF\n",
    "\n",
    "infos = []\n",
    "\n",
    "# 创建开启渲染的环境\n",
    "env = JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, variables = variables, spaces = spaces, verbose=True)\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = SAC.load(\"SAC_model\", env=env)\n",
    "\n",
    "# 视频帧列表\n",
    "frames = []\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# 保存为GIF（也可以保存为MP4）\n",
    "input_frame = \"images\"\n",
    "output_gif = \"train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)\n",
    "env.close()\n",
    "\n",
    "np.save(\"info_SAC.npy\", info[\"info\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5881cc034ba9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:03:18.150239Z",
     "start_time": "2025-08-06T10:03:17.033327Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "info = np.load(\"info_SAC.npy\", allow_pickle=True)\n",
    "# force = info\n",
    "force = [f[\"F\"] for f in info[0:]]\n",
    "y_force = [f[\"fluid_force_y\"] for f in info[0:]]\n",
    "x_force = [f[\"fluid_force_x\"] for f in info[0:]]\n",
    "y_dis = [f[\"y_dis\"] for f in info[0:]]\n",
    "x_dis = [f[\"x_dis\"] for f in info[0:]]\n",
    "\n",
    "# info2 = np.load(\"info2_SAC.npy\", allow_pickle=True)\n",
    "# y_dis2 = [f[\"y_dis\"] for f in info2[0:]]\n",
    "\n",
    "\n",
    "x = np.arange(len(y_force))\n",
    "# x2 = np.arange(len(y_dis2))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "# plt.plot(x, force, label=\"ratio\", color=\"red\")\n",
    "plt.plot(x, force, label=\"x_force\", color=\"red\")\n",
    "plt.plot(x, x_force, label=\"x_fluid\", color=\"blue\")\n",
    "plt.plot(x, x_dis, label=\"x_displacement\", color=\"green\")\n",
    "# plt.plot(x2, y_dis2, label=\"init_displacement\", color=\"yellow\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"force & displacement\")\n",
    "plt.title(\"Force and Displacement in x direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99718faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gif import create_GIF\n",
    "# 保存为GIF（也可以保存为MP4）\n",
    "input_frame = \"images\"\n",
    "output_gif = \"train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a3a4c72317778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx_no = np.load(\"none_train.npy\")\n",
    "nx_tr = np.load(\"train.npy\")\n",
    "y_cons = np.load(\"x_cons.npy\")\n",
    "\n",
    "x = np.arange(len(nx_no))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y_cons, label=\"Y_Constrained\", color=\"yellow\")\n",
    "plt.plot(x, nx_no, label=\"Init_VIV\", color=\"blue\")\n",
    "plt.plot(x, nx_tr, label=\"SAC_Trained\", color=\"red\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"X - Value\")\n",
    "plt.title(\"Comparison of Displacement in X-Direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd743b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from VIV_gym import VIVEnv\n",
    "# ===== Register to Gym =====\n",
    "gym.register(\n",
    "    id=\"VIV-v0\",\n",
    "    entry_point=JuliaEnv,\n",
    "    kwargs={\n",
    "        \"env\": VIVEnv,\n",
    "        \"statics\": {\"L_unit\": 16, \"F_scale\": 1.0, \"size\": (10, 8), \"location\": [3, 4]},  # 这里要替换成你实际的参数\n",
    "        \"variables\": {\"position\":[0.0, -1.0], \"velocity\":[0.0, 0.0]},\n",
    "        \"spaces\": {\"action\": 1, \"observation\": 3},\n",
    "        \"verbose\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14367287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"VIV-v0\")\n",
    "obs, info = env.reset()\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
