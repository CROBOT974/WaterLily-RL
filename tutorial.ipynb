{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1306c8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:01.082213Z",
     "start_time": "2025-08-28T10:02:49.597887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "\n",
    "os.environ[\"JULIA_NUM_THREADS\"] = \"8\"\n",
    "from julia import Julia\n",
    "jl = Julia(compiled_modules=False)\n",
    "\n",
    "from julia import Main\n",
    "print(Main.eval(\"Threads.nthreads()\"))\n",
    "from src.VIV_gym import JuliaEnv\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Lib支持\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, CallbackList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2fe78b430b301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:44.764763Z",
     "start_time": "2025-08-28T10:03:44.757683Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "反馈reward和建立checkpoint\n",
    "\n",
    "\"\"\"\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = None\n",
    "        self.episode_steps = []          # 存每个 episode 的 step 数\n",
    "        self.current_steps = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.current_rewards = np.zeros(self.training_env.num_envs)\n",
    "        self.current_steps = np.zeros(self.training_env.num_envs, dtype=int)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        self.current_rewards += rewards\n",
    "        self.current_steps += 1   # 每个 step 累加\n",
    "\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                self.episode_rewards.append(self.current_rewards[i])\n",
    "                self.episode_steps.append(self.current_steps[i])  # 记录步数\n",
    "\n",
    "                print(f\"Episode finished after {self.current_steps[i]} steps\")\n",
    "                print(f\"Episode reward: {self.current_rewards[i]:.2f}\")\n",
    "                # reset\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.current_steps[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq= 1000,\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7553a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:45.729164Z",
     "start_time": "2025-08-28T10:03:45.717647Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "训练用参数(VIV)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 16\n",
    "def pos_generator():\n",
    "    return [0.0, np.random.uniform(- diameter/6, diameter/6)]\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,\n",
    "    \"action_scale\": 50,\n",
    "    \"size\": [10, 8],\n",
    "    \"location\": [3, 4]\n",
    "}\n",
    "#variable parameters\n",
    "variables = {\n",
    "    \"position\":[0.0, diameter/6],\n",
    "    \"velocity\":[0.0, 0.0]\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,\n",
    "    \"observation\":3\n",
    "}\n",
    "\n",
    "from src.VIV_gym import VIVEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "训练用参数(FOIL)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 16\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,\n",
    "    \"F_scale\": 10,\n",
    "    \"size\": [8, 6],\n",
    "    \"nose\": [1, 4],\n",
    "    \"rot_center\":[0.25,0]\n",
    "}\n",
    "#variable parameters\n",
    "variables = {\n",
    "    \"position\":[0.0, 0.0],\n",
    "    \"velocity\":[0.0, 0.0],\n",
    "    \"theta\":0.05 * np.pi,\n",
    "    \"rot_vel\": 0.0,\n",
    "    \"rot_acc\": 0.0\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,\n",
    "    \"observation\":5\n",
    "}\n",
    "\n",
    "from src.VIV_gym import FoilEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "训练用参数(Drag)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 48\n",
    "\n",
    "def ksi_generator():\n",
    "    return np.random.uniform(3.0, 4.0)\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,\n",
    "    \"F_scale\": 8,\n",
    "    \"L_ratio\": 0.15,\n",
    "    \"L_gap\":0.05,\n",
    "    \"location\": [2, 0],\n",
    "    \"size\": [6, 2]\n",
    "}\n",
    "# variable parameters\n",
    "variables = {\n",
    "    \"ksi\": ksi_generator\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,\n",
    "    \"observation\":2\n",
    "}\n",
    "\n",
    "from src.VIV_gym import DragEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defbdcbcd7d8e0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:06:25.845138Z",
     "start_time": "2025-08-28T10:03:47.256889Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia VIV Environment initialized.\n",
      "Using cpu device\n",
      "Reward_Sum: -132.23\n",
      "done: True\n",
      "Episode finished after 799 steps\n",
      "Episode reward: -132.23\n",
      "Reward_Sum: -264.64\n",
      "done: True\n",
      "Episode finished after 799 steps\n",
      "Episode reward: -264.64\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 94   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 21   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m reward_callback \u001b[38;5;241m=\u001b[39m RewardLoggerCallback()\n\u001b[0;32m     16\u001b[0m callback \u001b[38;5;241m=\u001b[39m CallbackList([checkpoint_callback, reward_callback])\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(reward_callback\u001b[38;5;241m.\u001b[39mepisode_rewards)\n",
      "File \u001b[1;32md:\\python3.1.0\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python3.1.0\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\python3.1.0\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:275\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 275\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    277\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[1;32md:\\python3.1.0\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python3.1.0\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python3.1.0\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "单线程环境建立，训练，保持\n",
    "\n",
    "\"\"\"\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "model.learn(total_timesteps=100_000, callback = callback)\n",
    "model.save(\"PPO_model\")\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a89abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "单线程环境建立，训练，保持(注册表)\n",
    "\n",
    "\"\"\"\n",
    "env = DummyVecEnv([lambda: gym.make(\"VIV-v0\")])\n",
    "\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "model.learn(total_timesteps=2_000, callback = callback)\n",
    "model.save(\"PPO_model\")\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "多线程环境建立\n",
    "\n",
    "\"\"\"\n",
    "def make_env(rank: int, seed: int = 0):\n",
    "    def _init():\n",
    "        env = JuliaVIVEnv(render_mode=None, max_episode_steps=200, verbose=1)\n",
    "        env.reset(seed=seed+rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "num_envs = 4\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_envs)])\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "\n",
    "model.learn(total_timesteps=20_000, callback = callback)\n",
    "model.save(\"ppo_model\")\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf25679942131",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(reward_callback.episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e00a8abd73c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "加载checkpoint并继续训练\n",
    "\n",
    "\"\"\"\n",
    "def make_env(rank: int, seed: int = 0):\n",
    "    def _init():\n",
    "        env = JuliaDragEnv(render_mode=None, max_episode_steps=30, verbose=1)\n",
    "        env.reset(seed=seed+rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "num_envs = 4\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_envs)])\n",
    "\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "\n",
    "model = PPO.load(\"./checkpoints/ppo_model_40000_steps\", env=env, device='cpu')\n",
    "model.learn(total_timesteps=20_000, callback = callback)\n",
    "rewards_ex = np.array(reward_callback.episode_rewards)\n",
    "rewards = [rewards, rewards_ex]\n",
    "model.save(\"./model_stage/ppo_model_40k\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930ccd7507654ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T03:35:29.970382Z",
     "start_time": "2025-08-11T03:35:23.541861Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "绘图功能\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 参数：滑动窗口大小\n",
    "window = 10\n",
    "\n",
    "def plot_rewards(rewards, window=100):\n",
    "    episode = np.arange(len(rewards))\n",
    "\n",
    "    # 计算滑动均值和标准差\n",
    "    def moving_avg(x, w):\n",
    "        return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    mean = moving_avg(rewards, window)\n",
    "    std = np.array([\n",
    "        np.std(rewards[max(0, i - window + 1):i + 1])\n",
    "        for i in range(window - 1, len(rewards))\n",
    "    ])\n",
    "\n",
    "    # 对应 x 轴\n",
    "    x = np.arange(window - 1, len(rewards))\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, mean, label='Mean Reward')\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3, label='±1 Std Dev')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward over Training\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards,window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8e6b296ccfe1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-11T03:35:38.095121Z"
    }
   },
   "outputs": [],
   "source": [
    "#训练结束后出gif\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from model.VIV_gym import JuliaEnv\n",
    "from gif import create_GIF\n",
    "\n",
    "infos = []\n",
    "\n",
    "# 创建开启渲染的环境\n",
    "env = JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, variables = variables, spaces = spaces, verbose=True)\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = SAC.load(\"SAC_model\", env=env)\n",
    "\n",
    "# 视频帧列表\n",
    "frames = []\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# 保存为GIF（也可以保存为MP4）\n",
    "input_frame = \"images\"\n",
    "output_gif = \"train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)\n",
    "env.close()\n",
    "\n",
    "np.save(\"info_SAC.npy\", info[\"info\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5881cc034ba9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:03:18.150239Z",
     "start_time": "2025-08-06T10:03:17.033327Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "info = np.load(\"info_SAC.npy\", allow_pickle=True)\n",
    "# force = info\n",
    "force = [f[\"F\"] for f in info[0:]]\n",
    "y_force = [f[\"fluid_force_y\"] for f in info[0:]]\n",
    "x_force = [f[\"fluid_force_x\"] for f in info[0:]]\n",
    "y_dis = [f[\"y_dis\"] for f in info[0:]]\n",
    "x_dis = [f[\"x_dis\"] for f in info[0:]]\n",
    "\n",
    "# info2 = np.load(\"info2_SAC.npy\", allow_pickle=True)\n",
    "# y_dis2 = [f[\"y_dis\"] for f in info2[0:]]\n",
    "\n",
    "\n",
    "x = np.arange(len(y_force))\n",
    "# x2 = np.arange(len(y_dis2))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "# plt.plot(x, force, label=\"ratio\", color=\"red\")\n",
    "plt.plot(x, force, label=\"x_force\", color=\"red\")\n",
    "plt.plot(x, x_force, label=\"x_fluid\", color=\"blue\")\n",
    "plt.plot(x, x_dis, label=\"x_displacement\", color=\"green\")\n",
    "# plt.plot(x2, y_dis2, label=\"init_displacement\", color=\"yellow\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"force & displacement\")\n",
    "plt.title(\"Force and Displacement in x direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99718faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gif import create_GIF\n",
    "# 保存为GIF（也可以保存为MP4）\n",
    "input_frame = \"images\"\n",
    "output_gif = \"train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a3a4c72317778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx_no = np.load(\"none_train.npy\")\n",
    "nx_tr = np.load(\"train.npy\")\n",
    "y_cons = np.load(\"x_cons.npy\")\n",
    "\n",
    "x = np.arange(len(nx_no))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y_cons, label=\"Y_Constrained\", color=\"yellow\")\n",
    "plt.plot(x, nx_no, label=\"Init_VIV\", color=\"blue\")\n",
    "plt.plot(x, nx_tr, label=\"SAC_Trained\", color=\"red\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"X - Value\")\n",
    "plt.title(\"Comparison of Displacement in X-Direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd743b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from VIV_gym import VIVEnv\n",
    "# ===== Register to Gym =====\n",
    "gym.register(\n",
    "    id=\"VIV-v0\",\n",
    "    entry_point=JuliaEnv,\n",
    "    kwargs={\n",
    "        \"env\": VIVEnv,\n",
    "        \"statics\": {\"L_unit\": 16, \"F_scale\": 1.0, \"size\": (10, 8), \"location\": [3, 4]},  # 这里要替换成你实际的参数\n",
    "        \"variables\": {\"position\":[0.0, -1.0], \"velocity\":[0.0, 0.0]},\n",
    "        \"spaces\": {\"action\": 1, \"observation\": 3},\n",
    "        \"verbose\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14367287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"VIV-v0\")\n",
    "obs, info = env.reset()\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
