{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6062c",
   "metadata": {},
   "source": [
    "# WaterLily-RL Tutorial - Getting Started  教程 - 入门指南\n",
    "WaterLily-RL is a Deep Reinforcement Learning(DRL) simulation framework forcing on the fluid dyamics. \n",
    "It trains the agent in a vitual fluid environment created by WaterLily, a simple and fast fluid simulator.\n",
    "\n",
    "WaterLily-RL 是一个专注于流体动力学的​​深度强化学习仿真框架​​。它通过在 WaterLily（一个简单快速的流体模拟器）创建的虚拟流体环境中训练智能体（agent）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187365a",
   "metadata": {},
   "source": [
    "## Introduction  简介\n",
    "This tutorial provides a basic DRL task in fluid dynamics - a agent learns how to execute a direct force \n",
    "to restrain the virbration caused by incoming flow (Vortex-Induced Vibration).\n",
    "\n",
    "本教程提供了一个流体动力学中的基础 DRL 任务——训练一个智能体学习如何施加一个直接的作用力，\n",
    "以抑制来流引起的振动（涡激振动，Vortex-Induced Vibration）\n",
    "\n",
    "## Install Python Dependencies Using Pip  使用 Pip 安装 Python 依赖\n",
    "List of full Python dependencies can be found in the setup.py,follow the instruction in README can help you install them.\n",
    "\n",
    "完整的 Python 依赖列表可以在 setup.py文件中找到。按照 README 文件中的说明可以帮助你安装它们。\n",
    "\n",
    "## Install Julia Dependencies  安装 Julia 依赖\n",
    "List of full Julia dependencies can be found in the README.\n",
    "\n",
    "完整的 Julia 依赖列表可以在 README 文件中找到。\n",
    "希望这个翻译对你有帮助！如果你需要进一步的协助，请随时告诉我。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6d80e",
   "metadata": {},
   "source": [
    "## Multi-threads  多线程支持\n",
    "If your operating system is Linux, you can build multiply threads to accelerate the simulation. \n",
    "It is not recommanded to implement multi-threads in Windows, since it may result in issues in rendering.\n",
    "\n",
    "如果您的操作系统是 Linux，可以构建多线程来加速仿真。\n",
    "不建议在 Windows 系统中使用多线程功能，因为这可能会导致渲染问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JULIA_NUM_THREADS\"] = \"8\" # build 8 threads\n",
    "from julia import Julia\n",
    "jl = Julia(compiled_modules=False)\n",
    "from julia import Main\n",
    "print(Main.eval(\"Threads.nthreads()\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e1c25",
   "metadata": {},
   "source": [
    "## Imports 导入依赖库\n",
    "As we used Stable-Baselines3 as the benchmark DRL library, 'gymnasium' and 'stable_baselines3' \n",
    "are required. You can import the PPO here as the whole tutorial is based on it.\n",
    "\n",
    "由于我们采用 Stable-Baselines3 作为基准深度强化学习库，因此需要安装 'gymnasium' 和 'stable_baselines3' 这两个库。\n",
    "在本教程中您可以导入 PPO（近端策略优化）算法，因为整个教程都基于该算法实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306c8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:01.082213Z",
     "start_time": "2025-08-28T10:02:49.597887Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Lib 支持\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "from julia import Julia\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, CallbackList\n",
    "from src.gym_base import JuliaEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1b324",
   "metadata": {},
   "source": [
    "## Return the episodic reward and Set the checkpoint  返回周期奖励与设置检查点\n",
    "\n",
    "In order to deal with the interrupt during learning processing, or willing to continuing \n",
    "the completed training, you need to set the checkpoints and callback later.\n",
    "\n",
    "为应对学习过程中可能发生的中断情况，或希望延续已完成的训练进程，您需要设置检查点并在后续配置回调功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fe78b430b301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:44.764763Z",
     "start_time": "2025-08-28T10:03:44.757683Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "反馈reward和建立checkpoint\n",
    "\n",
    "\"\"\"\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = None\n",
    "        self.episode_steps = []          # 存每个 episode 的 step 数\n",
    "        self.current_steps = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.current_rewards = np.zeros(self.training_env.num_envs)\n",
    "        self.current_steps = np.zeros(self.training_env.num_envs, dtype=int)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        self.current_rewards += rewards\n",
    "        self.current_steps += 1   # 每个 step 累加\n",
    "\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                self.episode_rewards.append(self.current_rewards[i])\n",
    "                self.episode_steps.append(self.current_steps[i])  # 记录步数\n",
    "\n",
    "                print(f\"Episode finished after {self.current_steps[i]} steps\")\n",
    "                print(f\"Episode reward: {self.current_rewards[i]:.2f}\")\n",
    "                # reset\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.current_steps[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq= 10000,\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2638e",
   "metadata": {},
   "source": [
    "## Parameters 参数配置\n",
    "The three dict \"statics\", \"variables\"and \"spaces\" are essential parameters which should be\n",
    "determined before running. 'VIV_gym' from 'src' floder is the source code to refer the julia\n",
    "file.\n",
    "\n",
    "在运行前必须确定三个核心参数字典：\"statics\"（静态参数）、\"variables\"（动态变量）和 \"spaces\"（空间参数）。\n",
    "可通过 'src' 文件夹中的 'VIV_gym' 源代码文件参考对应的 Julia 文件实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7553a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:45.729164Z",
     "start_time": "2025-08-28T10:03:45.717647Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "训练用参数(VIV)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 16\n",
    "def pos_generator():\n",
    "    return [0.0, np.random.uniform(- diameter/6, diameter/6)]\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,                         # dimension of the object (diameter of the circle)\n",
    "    \"action_scale\": 50,                         # amplifing the action from [-1,1] to [-50,50]\n",
    "    \"size\": [10, 8],                            # size ratio of the simulation env\n",
    "    \"location\": [3, 4]                          # location of the object in the simulation env (size ratio)\n",
    "}\n",
    "#variable parameters\n",
    "variables = {\n",
    "    \"position\":[0.0, diameter/6],               # manual position of the object respects to the initial location\n",
    "    \"velocity\":[0.0, 0.0]                       # initial velocity\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,                                 # action space\n",
    "    \"observation\":3                             # observation spaces\n",
    "}\n",
    "\n",
    "from src.gym_base import VIVEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b42113",
   "metadata": {},
   "source": [
    "## Create the env and instantiate the agent  创建环境并实例化智能体\n",
    "### Refer the WaterLily env and build the model  引用 WaterLily 环境并构建模型\n",
    "For this example, we will use the basic VIV senario as the simulation env. Thus,\n",
    "we can set the 'VIVEnv' to 'env' and import the previously determined parameters\n",
    "(statics, variables and spaces). It is recommanded to set the 'render_mode' as \n",
    "'None', since create image will extremely slow down the train process.\n",
    "\n",
    "Then, as mentioned before, we accepct PPO algorthm and the policy is \"MlpPolicy\" \n",
    "since the input action is a vector.\n",
    "\n",
    "在本示例中，我们将使用基础的涡激振动（VIV）场景作为仿真环境。\n",
    "因此，可将 'VIVEnv' 设置为 'env' 并导入先前确定的参数\n",
    "（statics、variables 和 spaces）。建议将 'render_mode' 设置为\n",
    "'None'，因为创建图像会大幅降低训练速度。\n",
    "\n",
    "随后，如前所述，我们采用 PPO 算法并选择 \"MlpPolicy\" 策略，\n",
    "因为输入动作为向量形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69273035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the env and model\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4372c",
   "metadata": {},
   "source": [
    "let's define the callback function from previously determined checkpoint function.\n",
    "\n",
    "让我们通过先前定义的检查点函数来设置回调函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5884f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the checkpoint and reward sum in callback\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfa78a",
   "metadata": {},
   "source": [
    "### Train the agent and save it 训练并保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbdcbcd7d8e0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:06:25.845138Z",
     "start_time": "2025-08-28T10:03:47.256889Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# train and save the agent\n",
    "model.learn(total_timesteps=100_000, callback = callback)\n",
    "model.save(\"./model/PPO_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect te rewards and close the env\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "np.save('./result/data/rewards.npy', rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad478f",
   "metadata": {},
   "source": [
    "## Evaluate the train process  训练过程评估\n",
    "### Image  过程图例\n",
    "After collecting the rewards, you can draw the rewards during training process here.\n",
    "Apperantly, the reward doesn't converge and the performance is not well. Don't worry,\n",
    "we can continuously train it from the last checkpoint later.\n",
    "\n",
    "在收集奖励数据后，您可在此绘制训练过程中的奖励变化曲线。\n",
    "目前可见奖励尚未收敛，模型表现欠佳。无需担心，\n",
    "我们后续可以从最后一个检查点继续训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930ccd7507654ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T03:35:29.970382Z",
     "start_time": "2025-08-11T03:35:23.541861Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "绘图功能\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the rewards\n",
    "rewards = np.load('./result/data/rewards.npy')\n",
    "# param：the sliding window for means and std\n",
    "window = 10\n",
    "\n",
    "def plot_rewards(rewards, window=100):\n",
    "    # calculate the means and stds\n",
    "    def moving_avg(x, w):\n",
    "        return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    mean = moving_avg(rewards, window)\n",
    "    std = np.array([\n",
    "        np.std(rewards[max(0, i - window + 1):i + 1])\n",
    "        for i in range(window - 1, len(rewards))\n",
    "    ])\n",
    "\n",
    "    # x-axis\n",
    "    x = np.arange(window - 1, len(rewards))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, mean, label='Mean Reward')\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3, label='±1 Std Dev')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward over Training\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards,window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b0388",
   "metadata": {},
   "source": [
    "### GIF & info  动态演示与数据记录\n",
    "You probably wanna see the trained agent's motion in a grphic windows or a gif of \n",
    "the whole moving process. You can create the gif here by play the trained model in\n",
    "our sim env.\n",
    "\n",
    "Moreover, if you wanted to analyse the dynamics of the trained agent, you can save\n",
    "the information, determined in julia files, into a npy file.\n",
    "\n",
    "您可能希望查看已训练智能体在图形窗口中的运动表现，或是生成完整运动过程的GIF动画。您可以通过在仿真环境中运行训练好的模型，在此直接创建动态演示图。\n",
    "\n",
    "此外，若需分析已训练智能体的动力学特性，可将Julia文件中定义的相关信息保存为npy格式文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8e6b296ccfe1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-11T03:35:38.095121Z"
    }
   },
   "outputs": [],
   "source": [
    "# create gif after training\n",
    "from src.gif import create_GIF\n",
    "\n",
    "# same simulation env while 'render_mode' is 'rgb_array' to create images\n",
    "env = JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, variables = variables, spaces = spaces, verbose=True)\n",
    "\n",
    "# load the trained PPO_model\n",
    "model = PPO.load(\"./model/PPO_model\", env=env)\n",
    "\n",
    "# video frame\n",
    "frames = []\n",
    "\n",
    "# reset the env\n",
    "print(\"测试\", env.reset())\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# if 'not done', then continue to perform the simulation operation based on trained model\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# save as gif\n",
    "input_frame = \"images\"\n",
    "output_gif = \"./result/gif/train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)\n",
    "env.close()\n",
    "\n",
    "# save the info\n",
    "np.save(\"./result/data/info_PPO.npy\", info[\"info\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a087965",
   "metadata": {},
   "source": [
    "### Analysis  可视化分析\n",
    "Now you have a npy file which contains all dynamic information, so you can plow a \n",
    "line chart of fluid force, agent applied force and the displacement in y direction.\n",
    "The result is not appropriate, as the train is not 100% converged, you can run the \n",
    "callback this time to continue the training.\n",
    "\n",
    "现在您已获取包含所有动力学信息的npy文件，可绘制流体作用力、智能体施加力以及y方向位移的线性图表。\n",
    "由于当前训练未完全收敛，结果尚未达到理想状态。您此时可运行回调功能以继续训练进程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5881cc034ba9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:03:18.150239Z",
     "start_time": "2025-08-06T10:03:17.033327Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "info = np.load(\"./result/data/info_PPO.npy\", allow_pickle=True)\n",
    "force = [f[\"F\"] for f in info[50:]]\n",
    "y_force = [f[\"fluid_force_y\"] for f in info[50:]]\n",
    "x_force = [f[\"fluid_force_x\"] for f in info[50:]]\n",
    "y_dis = [f[\"y_dis\"] for f in info[50:]]\n",
    "x_dis = [f[\"x_dis\"] for f in info[50:]]\n",
    "\n",
    "x = np.arange(len(y_force))\n",
    "# x2 = np.arange(len(y_dis2))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, force, label=\"y_force\", color=\"red\")\n",
    "plt.plot(x, y_force, label=\"y_fluid\", color=\"blue\")\n",
    "plt.plot(x, y_dis, label=\"y_displacement\", color=\"green\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"force & displacement\")\n",
    "plt.title(\"Force and Displacement in y direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fc109",
   "metadata": {},
   "source": [
    "## Callback  回调训练\n",
    "You found out that the train doesn't end up with a converged reward. You can\n",
    "start a subsequent process from the checkpoint of 100k steps. Run the other\n",
    "200k steps train now. After all, a converged sum of reward should be around -10.\n",
    "\n",
    "发现当前训练未获得收敛的奖励值。您可以从10万步的检查点启动后续训练流程，继续运行20万步的训练任务。最终收敛的累计奖励值应稳定在-10左右。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a873266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "加载checkpoint并继续训练\n",
    "\n",
    "\"\"\"\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "\n",
    "model = PPO.load(\"./checkpoints/ppo_model_100000_steps\", env=env, device='cpu')\n",
    "model.learn(total_timesteps=200_000, callback = callback)\n",
    "rewards_ex = np.array(reward_callback.episode_rewards)\n",
    "rewards = np.load('./result/data/rewards.npy')\n",
    "rewards = np.concatenate([rewards, rewards_ex])\n",
    "np.save('./result/data/rewards.npy', rewards)\n",
    "model.save(\"./model/PPO_model_100k-300k\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a769d4f",
   "metadata": {},
   "source": [
    "Plot the means and stds of the rewards now. You can find it is nearly converged.\n",
    "Apparently, the agent learned a policy to execute the force to counteract the lift \n",
    "force, keeps the agent around y=0.\n",
    "\n",
    "现在绘制奖励的均值和标准差图表，可以发现结果已接近收敛状态。\n",
    "显然，智能体已学会通过施加作用力来抵消升力的策略，能够将自身位置稳定在y=0附近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Reward Image\n",
    "\n",
    "\"\"\"\n",
    "# load the rewards\n",
    "rewards = np.load('./result/data/rewards.npy')\n",
    "# param：the sliding window for means and std\n",
    "window = 10\n",
    "\n",
    "def plot_rewards(rewards, window=100):\n",
    "    episode = np.arange(len(rewards))\n",
    "\n",
    "    # calculate the means and stds\n",
    "    def moving_avg(x, w):\n",
    "        return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    mean = moving_avg(rewards, window)\n",
    "    std = np.array([\n",
    "        np.std(rewards[max(0, i - window + 1):i + 1])\n",
    "        for i in range(window - 1, len(rewards))\n",
    "    ])\n",
    "\n",
    "    # x-axis\n",
    "    x = np.arange(window - 1, len(rewards))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, mean, label='Mean Reward')\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3, label='±1 Std Dev')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward over 250k Steps Training\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards,window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "GIF\n",
    "\n",
    "\"\"\"\n",
    "# same simulation env while 'render_mode' is 'rgb_array' to create images\n",
    "env = JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, variables = variables, spaces = spaces, verbose=True)\n",
    "\n",
    "# load the trained PPO_model\n",
    "model = PPO.load(\"./model/PPO_model_100k-300k\", env=env)\n",
    "\n",
    "# video frame\n",
    "frames = []\n",
    "\n",
    "# reset the env\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# if 'not done', then continue to perform the simulation operation based on trained model\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# save as gif\n",
    "input_frame = \"images\"\n",
    "output_gif = \"./result/gif/train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)\n",
    "env.close()\n",
    "\n",
    "# save the info\n",
    "np.save(\"./result/data/info_PPO.npy\", info[\"info\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329182e",
   "metadata": {},
   "source": [
    "You will find that the agent is completely trained, and the y-displacement are\n",
    "nearly 0 all time.\n",
    "\n",
    "您将会发现智能体已完成训练，y方向位移始终保持在接近零的水平。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e75131",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = np.load(\"./result/data/info_PPO.npy\", allow_pickle=True)\n",
    "force = [f[\"F\"] for f in info[50:]]\n",
    "y_force = [f[\"fluid_force_y\"] for f in info[50:]]\n",
    "x_force = [f[\"fluid_force_x\"] for f in info[50:]]\n",
    "y_dis = [f[\"y_dis\"] for f in info[50:]]\n",
    "x_dis = [f[\"x_dis\"] for f in info[50:]]\n",
    "\n",
    "x = np.arange(len(y_force))\n",
    "# x2 = np.arange(len(y_dis2))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, force, label=\"y_force\", color=\"red\")\n",
    "plt.plot(x, y_force, label=\"y_fluid\", color=\"blue\")\n",
    "plt.plot(x, y_dis, label=\"y_displacement\", color=\"green\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"force & displacement\")\n",
    "plt.title(\"Force and Displacement in y direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
