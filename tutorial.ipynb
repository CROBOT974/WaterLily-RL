{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6062c",
   "metadata": {},
   "source": [
    "# WaterLily-RL Tutorial - Getting Started\n",
    "WaterLily-RL is a Deep Reinforcement Learning(DRL) simulation framework forcing on the fluid dyamics. \n",
    "It trains the agent in a vitual fluid environment created by WaterLily, a simple and fast fluid simulator.\n",
    "\n",
    "## Introduction\n",
    "This tutorial provides a basic DRL task in fluid dynamics - a agent learns how to execute a direct force \n",
    "to restrain the virbration caused by incoming flow (Vortex-Induced Vibration).\n",
    "\n",
    "## Install Python Dependencies Using Pip\n",
    "List of full Python dependencies can be found in the setup.py,follow the instruction in README can help you install them.\n",
    "\n",
    "## Install Julia Dependencies\n",
    "List of full Julia dependencies can be found in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6d80e",
   "metadata": {},
   "source": [
    "## Multi-threads\n",
    "If your operating system is Linux, you can build multiply threads to accelerate the simulation. \n",
    "It is notrecommanded to implement multi-threads in Windows, since it may result in issues in rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JULIA_NUM_THREADS\"] = \"8\" # build 8 threads\n",
    "from julia import Julia\n",
    "jl = Julia(compiled_modules=False)\n",
    "from julia import Main\n",
    "print(Main.eval(\"Threads.nthreads()\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e1c25",
   "metadata": {},
   "source": [
    "## Imports\n",
    "As we used Stable-Baselines3 as the benchmark DRL library, 'gymnasium' and 'stable_baselines3' \n",
    "are required. You can import the PPO here as the whole tutorial is based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306c8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:01.082213Z",
     "start_time": "2025-08-28T10:02:49.597887Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Lib 支持\n",
    "\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, CallbackList\n",
    "from src.VIV_gym import JuliaEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1b324",
   "metadata": {},
   "source": [
    "## Return the episodic reward and Set the checkpoint\n",
    "\n",
    "In order to deal with the interrupt during learning processing, or willing to continuing \n",
    "the completed training, you need to set the checkpoints and callback later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fe78b430b301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:44.764763Z",
     "start_time": "2025-08-28T10:03:44.757683Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "反馈reward和建立checkpoint\n",
    "\n",
    "\"\"\"\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = None\n",
    "        self.episode_steps = []          # 存每个 episode 的 step 数\n",
    "        self.current_steps = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.current_rewards = np.zeros(self.training_env.num_envs)\n",
    "        self.current_steps = np.zeros(self.training_env.num_envs, dtype=int)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        self.current_rewards += rewards\n",
    "        self.current_steps += 1   # 每个 step 累加\n",
    "\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                self.episode_rewards.append(self.current_rewards[i])\n",
    "                self.episode_steps.append(self.current_steps[i])  # 记录步数\n",
    "\n",
    "                print(f\"Episode finished after {self.current_steps[i]} steps\")\n",
    "                print(f\"Episode reward: {self.current_rewards[i]:.2f}\")\n",
    "                # reset\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.current_steps[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq= 10000,\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2638e",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "The three dict \"statics\", \"variables\"and \"spaces\" are essential parameters which should be\n",
    "determined before running. 'VIV_gym' from 'src' floder is the source code to refer the julia\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7553a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:45.729164Z",
     "start_time": "2025-08-28T10:03:45.717647Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "训练用参数(VIV)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 16\n",
    "def pos_generator():\n",
    "    return [0.0, np.random.uniform(- diameter/6, diameter/6)]\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,                         # dimension of the object (diameter of the circle)\n",
    "    \"action_scale\": 50,                         # amplifing the action from [-1,1] to [-50,50]\n",
    "    \"size\": [10, 8],                            # size ratio of the simulation env\n",
    "    \"location\": [3, 4]                          # location of the object in the simulation env (size ratio)\n",
    "}\n",
    "#variable parameters\n",
    "variables = {\n",
    "    \"position\":[0.0, diameter/6],               # manual position of the object respects to the initial location\n",
    "    \"velocity\":[0.0, 0.0]                       # initial velocity\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,                                 # action space\n",
    "    \"observation\":3                             # observation spaces\n",
    "}\n",
    "\n",
    "from src.VIV_gym import VIVEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b42113",
   "metadata": {},
   "source": [
    "## Create the env and instantiate the agent\n",
    "### Refer the WaterLily env and build the model\n",
    "For this example, we will use the basic VIV senario as the simulation env. Thus,\n",
    "we can set the 'VIVEnv' to 'env' and import the previously determined parameters\n",
    "(statics, variables and spaces). It is recommanded to set the 'render_mode' as \n",
    "'None', since create image will extremely slow down the train process.\n",
    "\n",
    "Then, as mentioned before, we accepct PPO algorthm and the policy is \"MlpPolicy\" \n",
    "since the input action is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69273035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the env and model\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4372c",
   "metadata": {},
   "source": [
    "let's define the callback function from previously determined checkpoint function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5884f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the checkpoint and reward sum in callback\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfa78a",
   "metadata": {},
   "source": [
    "### Train the agent and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbdcbcd7d8e0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:06:25.845138Z",
     "start_time": "2025-08-28T10:03:47.256889Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# train and save the agent\n",
    "model.learn(total_timesteps=100_000, callback = callback)\n",
    "model.save(\"./model/PPO_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect te rewards and close the env\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "np.save('rewards.npy', rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad478f",
   "metadata": {},
   "source": [
    "## Evaluate the train process\n",
    "### Image\n",
    "After collecting the rewards, you can draw the rewards during training process here.\n",
    "Apperantly, the reward doesn't converge and the performance is not well. Don't worry,\n",
    "we can continuously train it from the last checkpoint later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930ccd7507654ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T03:35:29.970382Z",
     "start_time": "2025-08-11T03:35:23.541861Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "绘图功能\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# load the rewards\n",
    "rewards = np.load('rewards.npy')\n",
    "# param：the sliding window for means and std\n",
    "window = 10\n",
    "\n",
    "def plot_rewards(rewards, window=100):\n",
    "    # calculate the means and stds\n",
    "    def moving_avg(x, w):\n",
    "        return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    mean = moving_avg(rewards, window)\n",
    "    std = np.array([\n",
    "        np.std(rewards[max(0, i - window + 1):i + 1])\n",
    "        for i in range(window - 1, len(rewards))\n",
    "    ])\n",
    "\n",
    "    # x-axis\n",
    "    x = np.arange(window - 1, len(rewards))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, mean, label='Mean Reward')\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3, label='±1 Std Dev')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward over Training\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards,window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b0388",
   "metadata": {},
   "source": [
    "### GIF & info\n",
    "You probably wanna see the trained agent's motion in a grphic windows or a gif of \n",
    "the whole moving process. You can create the gif here by play the trained model in\n",
    "our sim env.\n",
    "\n",
    "Moreover, if you wanted to analyse the dynamics of the trained agent, you can save\n",
    "the information, determined in julia files, into a npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8e6b296ccfe1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-11T03:35:38.095121Z"
    }
   },
   "outputs": [],
   "source": [
    "# create gif after training\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from src.VIV_gym import JuliaEnv\n",
    "from src.gif import create_GIF\n",
    "\n",
    "infos = []\n",
    "\n",
    "# same simulation env while 'render_mode' is 'rgb_array' to create images\n",
    "env = JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, variables = variables, spaces = spaces, verbose=True)\n",
    "\n",
    "# load the trained PPO_model\n",
    "model = PPO.load(\"./model/PPO_model\", env=env)\n",
    "\n",
    "# video frame\n",
    "frames = []\n",
    "\n",
    "# reset the env\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# if 'not done', then continue to perform the simulation operation based on trained model\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# save as gif\n",
    "input_frame = \"images\"\n",
    "output_gif = \"./result/train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)\n",
    "env.close()\n",
    "\n",
    "# save the info\n",
    "np.save(\"info_PPO.npy\", info[\"info\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a087965",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Now you have a npy file which contains all dynamic information, so you can plow a \n",
    "line chart of fluid force, agent applied force and the displacement in y direction.\n",
    "The result is not appropriate, as the train is not 100% converged, you can run the \n",
    "callback this time to continue the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5881cc034ba9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:03:18.150239Z",
     "start_time": "2025-08-06T10:03:17.033327Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "info = np.load(\"info_PPO.npy\", allow_pickle=True)\n",
    "force = [f[\"F\"] for f in info[50:]]\n",
    "y_force = [f[\"fluid_force_y\"] for f in info[50:]]\n",
    "x_force = [f[\"fluid_force_x\"] for f in info[50:]]\n",
    "y_dis = [f[\"y_dis\"] for f in info[50:]]\n",
    "x_dis = [f[\"x_dis\"] for f in info[50:]]\n",
    "\n",
    "x = np.arange(len(y_force))\n",
    "# x2 = np.arange(len(y_dis2))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, force, label=\"y_force\", color=\"red\")\n",
    "plt.plot(x, y_force, label=\"y_fluid\", color=\"blue\")\n",
    "plt.plot(x, y_dis, label=\"y_displacement\", color=\"green\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"force & displacement\")\n",
    "plt.title(\"Force and Displacement in y direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fc109",
   "metadata": {},
   "source": [
    "## Callback\n",
    "You found out that the train doesn't end up with a converged reward. You can\n",
    "start a subsequent process from the checkpoint of 100k steps. Run the other\n",
    "200k steps train now:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a873266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "加载checkpoint并继续训练\n",
    "\n",
    "\"\"\"\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "\n",
    "model = PPO.load(\"./checkpoints/ppo_model_100000_steps\", env=env, device='cpu')\n",
    "model.learn(total_timesteps=200_000, callback = callback)\n",
    "rewards_ex = np.array(reward_callback.episode_rewards)\n",
    "rewards = np.load('rewards.npy')\n",
    "rewards = np.concatenate([rewards, rewards_ex])\n",
    "np.save('rewards.npy', rewards)\n",
    "model.save(\"./model/PPO_model_100k-300k\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a769d4f",
   "metadata": {},
   "source": [
    "Plot the means and stds of the rewards now. You can find it is nearly converged.\n",
    "Apparently, the agent learned a policy to execute the force to counteract the lift \n",
    "force, keeps the agent around y=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Reward Image\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# load the rewards\n",
    "rewards = np.load('rewards.npy')\n",
    "# param：the sliding window for means and std\n",
    "window = 10\n",
    "\n",
    "def plot_rewards(rewards, window=100):\n",
    "    episode = np.arange(len(rewards))\n",
    "\n",
    "    # calculate the means and stds\n",
    "    def moving_avg(x, w):\n",
    "        return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    mean = moving_avg(rewards, window)\n",
    "    std = np.array([\n",
    "        np.std(rewards[max(0, i - window + 1):i + 1])\n",
    "        for i in range(window - 1, len(rewards))\n",
    "    ])\n",
    "\n",
    "    # x-axis\n",
    "    x = np.arange(window - 1, len(rewards))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, mean, label='Mean Reward')\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3, label='±1 Std Dev')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward over 250k Steps Training\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards,window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "GIF\n",
    "\n",
    "\"\"\"\n",
    "# create gif after training\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from src.VIV_gym import JuliaEnv\n",
    "from src.gif import create_GIF\n",
    "\n",
    "infos = []\n",
    "\n",
    "# same simulation env while 'render_mode' is 'rgb_array' to create images\n",
    "env = JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, variables = variables, spaces = spaces, verbose=True)\n",
    "\n",
    "# load the trained PPO_model\n",
    "model = PPO.load(\"./model/PPO_model_100k-200k\", env=env)\n",
    "\n",
    "# video frame\n",
    "frames = []\n",
    "\n",
    "# reset the env\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# if 'not done', then continue to perform the simulation operation based on trained model\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# save as gif\n",
    "input_frame = \"images\"\n",
    "output_gif = \"./result/train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)\n",
    "env.close()\n",
    "\n",
    "# save the info\n",
    "np.save(\"info_PPO.npy\", info[\"info\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329182e",
   "metadata": {},
   "source": [
    "You will find that the agent is completely trained, and the y-displacement are\n",
    "nearly 0 all time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e75131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "info = np.load(\"info_PPO.npy\", allow_pickle=True)\n",
    "force = [f[\"F\"] for f in info[50:]]\n",
    "y_force = [f[\"fluid_force_y\"] for f in info[50:]]\n",
    "x_force = [f[\"fluid_force_x\"] for f in info[50:]]\n",
    "y_dis = [f[\"y_dis\"] for f in info[50:]]\n",
    "x_dis = [f[\"x_dis\"] for f in info[50:]]\n",
    "\n",
    "x = np.arange(len(y_force))\n",
    "# x2 = np.arange(len(y_dis2))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, force, label=\"y_force\", color=\"red\")\n",
    "plt.plot(x, y_force, label=\"y_fluid\", color=\"blue\")\n",
    "plt.plot(x, y_dis, label=\"y_displacement\", color=\"green\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"force & displacement\")\n",
    "plt.title(\"Force and Displacement in y direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
