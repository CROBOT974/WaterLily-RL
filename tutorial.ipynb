{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6062c",
   "metadata": {},
   "source": [
    "# ğŸŒŠDeep Reinforcement Learning with WaterLily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1306c8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:01.082213Z",
     "start_time": "2025-08-28T10:02:49.597887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "\n",
    "os.environ[\"JULIA_NUM_THREADS\"] = \"8\"\n",
    "from julia import Julia\n",
    "jl = Julia(compiled_modules=False)\n",
    "\n",
    "from julia import Main\n",
    "print(Main.eval(\"Threads.nthreads()\"))\n",
    "from src.VIV_gym import JuliaEnv\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Libæ”¯æŒ\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, CallbackList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2fe78b430b301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:44.764763Z",
     "start_time": "2025-08-28T10:03:44.757683Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "åé¦ˆrewardå’Œå»ºç«‹checkpoint\n",
    "\n",
    "\"\"\"\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = None\n",
    "        self.episode_steps = []          # å­˜æ¯ä¸ª episode çš„ step æ•°\n",
    "        self.current_steps = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.current_rewards = np.zeros(self.training_env.num_envs)\n",
    "        self.current_steps = np.zeros(self.training_env.num_envs, dtype=int)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        self.current_rewards += rewards\n",
    "        self.current_steps += 1   # æ¯ä¸ª step ç´¯åŠ \n",
    "\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                self.episode_rewards.append(self.current_rewards[i])\n",
    "                self.episode_steps.append(self.current_steps[i])  # è®°å½•æ­¥æ•°\n",
    "\n",
    "                print(f\"Episode finished after {self.current_steps[i]} steps\")\n",
    "                print(f\"Episode reward: {self.current_rewards[i]:.2f}\")\n",
    "                # reset\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.current_steps[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq= 10000,\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7553a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:45.729164Z",
     "start_time": "2025-08-28T10:03:45.717647Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "è®­ç»ƒç”¨å‚æ•°(VIV)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 16\n",
    "def pos_generator():\n",
    "    return [0.0, np.random.uniform(- diameter/6, diameter/6)]\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,\n",
    "    \"action_scale\": 50,\n",
    "    \"size\": [10, 8],\n",
    "    \"location\": [3, 4]\n",
    "}\n",
    "#variable parameters\n",
    "variables = {\n",
    "    \"position\":[0.0, diameter/6],\n",
    "    \"velocity\":[0.0, 0.0]\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,\n",
    "    \"observation\":3\n",
    "}\n",
    "\n",
    "from src.VIV_gym import VIVEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbdcbcd7d8e0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:06:25.845138Z",
     "start_time": "2025-08-28T10:03:47.256889Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "å•çº¿ç¨‹ç¯å¢ƒå»ºç«‹ï¼Œè®­ç»ƒï¼Œä¿æŒ\n",
    "\n",
    "\"\"\"\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "model.learn(total_timesteps=300_000, callback = callback)\n",
    "model.save(\"./model/PPO_model\")\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "np.save('rewards.npy', rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "å¤šçº¿ç¨‹ç¯å¢ƒå»ºç«‹\n",
    "\n",
    "\"\"\"\n",
    "def make_env(rank: int, seed: int = 0):\n",
    "    def _init():\n",
    "        env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                            variables = variables, spaces = spaces, verbose=1)])\n",
    "        env.reset(seed=seed+rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "num_envs = 4\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_envs)])\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "model.learn(total_timesteps=20_000, callback = callback)\n",
    "model.save(\"./model/PPO_model\")\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "np.save('rewards.npy', rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e00a8abd73c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "åŠ è½½checkpointå¹¶ç»§ç»­è®­ç»ƒ\n",
    "\n",
    "\"\"\"\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "\n",
    "model = PPO.load(\"./checkpoints/ppo_model_300000_steps\", env=env, device='cpu')\n",
    "model.learn(total_timesteps=10_000, callback = callback)\n",
    "rewards_ex = np.array(reward_callback.episode_rewards)\n",
    "rewards = np.load('rewards.npy')\n",
    "rewards = [rewards, rewards_ex]\n",
    "np.save('rewards.npy', rewards)\n",
    "model.save(\"./model/PPO_model_300k-310k\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930ccd7507654ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T03:35:29.970382Z",
     "start_time": "2025-08-11T03:35:23.541861Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "ç»˜å›¾åŠŸèƒ½\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rewards = np.load('rewards.npy')\n",
    "# å‚æ•°ï¼šæ»‘åŠ¨çª—å£å¤§å°\n",
    "window = 10\n",
    "\n",
    "def plot_rewards(rewards, window=100):\n",
    "    episode = np.arange(len(rewards))\n",
    "\n",
    "    # è®¡ç®—æ»‘åŠ¨å‡å€¼å’Œæ ‡å‡†å·®\n",
    "    def moving_avg(x, w):\n",
    "        return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    mean = moving_avg(rewards, window)\n",
    "    std = np.array([\n",
    "        np.std(rewards[max(0, i - window + 1):i + 1])\n",
    "        for i in range(window - 1, len(rewards))\n",
    "    ])\n",
    "\n",
    "    # å¯¹åº” x è½´\n",
    "    x = np.arange(window - 1, len(rewards))\n",
    "\n",
    "    # ç»˜å›¾\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, mean, label='Mean Reward')\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3, label='Â±1 Std Dev')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward over Training\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards,window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d8e6b296ccfe1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-11T03:35:38.095121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia VIV Environment initialized.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Reward_Sum: -9.60\n",
      "done: True\n"
     ]
    }
   ],
   "source": [
    "#è®­ç»ƒç»“æŸåå‡ºgif\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from src.VIV_gym import JuliaEnv\n",
    "from src.gif import create_GIF\n",
    "\n",
    "infos = []\n",
    "\n",
    "# åˆ›å»ºå¼€å¯æ¸²æŸ“çš„ç¯å¢ƒ\n",
    "env = JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, variables = variables, spaces = spaces, verbose=True)\n",
    "\n",
    "# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "model = PPO.load(\"./model/PPO_model\", env=env)\n",
    "\n",
    "# è§†é¢‘å¸§åˆ—è¡¨\n",
    "frames = []\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# ä¿å­˜ä¸ºGIFï¼ˆä¹Ÿå¯ä»¥ä¿å­˜ä¸ºMP4ï¼‰\n",
    "input_frame = \"images\"\n",
    "output_gif = \"./result/train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)\n",
    "env.close()\n",
    "\n",
    "np.save(\"info_PPO.npy\", info[\"info\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5881cc034ba9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:03:18.150239Z",
     "start_time": "2025-08-06T10:03:17.033327Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "info = np.load(\"info_PPO.npy\", allow_pickle=True)\n",
    "force = [f[\"F\"] for f in info[50:]]\n",
    "y_force = [f[\"fluid_force_y\"] for f in info[50:]]\n",
    "x_force = [f[\"fluid_force_x\"] for f in info[50:]]\n",
    "y_dis = [f[\"y_dis\"] for f in info[50:]]\n",
    "x_dis = [f[\"x_dis\"] for f in info[50:]]\n",
    "\n",
    "x = np.arange(len(y_force))\n",
    "# x2 = np.arange(len(y_dis2))\n",
    "\n",
    "# ç”»å›¾\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, force, label=\"y_force\", color=\"red\")\n",
    "plt.plot(x, y_force, label=\"y_fluid\", color=\"blue\")\n",
    "plt.plot(x, y_dis, label=\"y_displacement\", color=\"green\")\n",
    "\n",
    "# å›¾ä¾‹ã€æ ‡ç­¾ã€æ ‡é¢˜\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"force & displacement\")\n",
    "plt.title(\"Force and Displacement in y direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
