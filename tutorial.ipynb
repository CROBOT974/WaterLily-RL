{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce6062c",
   "metadata": {},
   "source": [
    "# WaterLily-RL Tutorial - Getting Started\n",
    "WaterLily-RL is a Deep Reinforcement Learning(DRL) simulation framework forcing on the fluid dyamics. \n",
    "It trains the agent in a vitual fluid environment created by WaterLily, a simple and fast fluid simulator.\n",
    "\n",
    "## Introduction\n",
    "This tutorial provides a basic DRL task in fluid dynamics - a agent learns how to execute a direct force \n",
    "to restrain the virbration caused by incoming flow (Vortex-Induced Vibration).\n",
    "\n",
    "## Install Python Dependencies Using Pip\n",
    "List of full Python dependencies can be found in the setup.py,follow the instruction in README can help you install them.\n",
    "\n",
    "## Install Julia Dependencies\n",
    "List of full Julia dependencies can be found in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6d80e",
   "metadata": {},
   "source": [
    "## Multi-threads\n",
    "If your operating system is Linux, you can build multiply threads to accelerate the simulation. \n",
    "It is notrecommanded to implement multi-threads in Windows, since it may result in issues in rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9bea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JULIA_NUM_THREADS\"] = \"8\" # build 8 threads\n",
    "from julia import Julia\n",
    "jl = Julia(compiled_modules=False)\n",
    "from julia import Main\n",
    "print(Main.eval(\"Threads.nthreads()\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e1c25",
   "metadata": {},
   "source": [
    "## Imports\n",
    "As we used Stable-Baselines3 as the benchmark DRL library, 'gymnasium' and 'stable_baselines3' \n",
    "are required. You can import the PPO here as the whole tutorial is based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1306c8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:01.082213Z",
     "start_time": "2025-08-28T10:02:49.597887Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Lib 支持\n",
    "\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, CallbackList\n",
    "from src.VIV_gym import JuliaEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1b324",
   "metadata": {},
   "source": [
    "## Return the episodic reward and Set the checkpoint\n",
    "\n",
    "In order to deal with the interrupt during learning processing, or willing to continuing \n",
    "the completed training, you need to set the checkpoints and callback later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2fe78b430b301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:44.764763Z",
     "start_time": "2025-08-28T10:03:44.757683Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "反馈reward和建立checkpoint\n",
    "\n",
    "\"\"\"\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = None\n",
    "        self.episode_steps = []          # 存每个 episode 的 step 数\n",
    "        self.current_steps = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.current_rewards = np.zeros(self.training_env.num_envs)\n",
    "        self.current_steps = np.zeros(self.training_env.num_envs, dtype=int)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "        self.current_rewards += rewards\n",
    "        self.current_steps += 1   # 每个 step 累加\n",
    "\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                self.episode_rewards.append(self.current_rewards[i])\n",
    "                self.episode_steps.append(self.current_steps[i])  # 记录步数\n",
    "\n",
    "                print(f\"Episode finished after {self.current_steps[i]} steps\")\n",
    "                print(f\"Episode reward: {self.current_rewards[i]:.2f}\")\n",
    "                # reset\n",
    "                self.current_rewards[i] = 0.0\n",
    "                self.current_steps[i] = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq= 10000,\n",
    "    save_path=\"./checkpoints/\",\n",
    "    name_prefix=\"ppo_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2638e",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "The three dict \"statics\", \"variables\"and \"spaces\" are essential parameters which should be\n",
    "determined before running. 'VIV_gym' from 'src' floder is the source code to refer the julia\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7553a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:03:45.729164Z",
     "start_time": "2025-08-28T10:03:45.717647Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "训练用参数(VIV)\n",
    "\n",
    "\"\"\"\n",
    "diameter = 16\n",
    "def pos_generator():\n",
    "    return [0.0, np.random.uniform(- diameter/6, diameter/6)]\n",
    "\n",
    "# static parameters\n",
    "statics = {\n",
    "    \"L_unit\": diameter,                         # dimension of the object (diameter of the circle)\n",
    "    \"action_scale\": 50,                         # amplifing the action from [-1,1] to [-50,50]\n",
    "    \"size\": [10, 8],                            # size ratio of the simulation env\n",
    "    \"location\": [3, 4]                          # location of the object in the simulation env (size ratio)\n",
    "}\n",
    "#variable parameters\n",
    "variables = {\n",
    "    \"position\":[0.0, diameter/6],               # manual position of the object respects to the initial location\n",
    "    \"velocity\":[0.0, 0.0]                       # initial velocity\n",
    "}\n",
    "# size of action sapce and observation spaces\n",
    "spaces = {\n",
    "    \"action\":1,                                 # action space\n",
    "    \"observation\":3                             # observation spaces\n",
    "}\n",
    "\n",
    "from src.VIV_gym import VIVEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b42113",
   "metadata": {},
   "source": [
    "## Create the env and instantiate the agent\n",
    "### Refer the WaterLily env and build the model\n",
    "For this example, we will use the basic VIV senario as the simulation env. Thus,\n",
    "we can set the 'VIVEnv' to 'env' and import the previously determined parameters\n",
    "(statics, variables and spaces). It is recommanded to set the 'render_mode' as \n",
    "'None', since create image will extremely slow down the train process.\n",
    "\n",
    "Then, as mentioned before, we accepct PPO algorthm and the policy is \"MlpPolicy\" \n",
    "since the input action is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69273035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia VIV Environment initialized.\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Build the env and model\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device = 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4372c",
   "metadata": {},
   "source": [
    "let's define the callback function from previously determined checkpoint function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae5884f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the checkpoint and reward sum in callback\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfa78a",
   "metadata": {},
   "source": [
    "### Train the agent and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "defbdcbcd7d8e0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T10:06:25.845138Z",
     "start_time": "2025-08-28T10:03:47.256889Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward_Sum: -509.31\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -509.31\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 82   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 24   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Reward_Sum: -629.02\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -629.02\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 85            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 48            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00095735886 |\n",
      "|    clip_fraction        | 0.00835       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | -0.0186       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.23          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.0014       |\n",
      "|    std                  | 0.976         |\n",
      "|    value_loss           | 10.4          |\n",
      "-------------------------------------------\n",
      "Reward_Sum: -538.46\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -538.46\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003510003 |\n",
      "|    clip_fraction        | 0.00923      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.292        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.14         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000442    |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 11.1         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -603.46\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -603.46\n",
      "Reward_Sum: -558.52\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -558.52\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015876055 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.485        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.03         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 10.7         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -535.91\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -535.91\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040954268 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.685        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.67         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 10.3         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -457.40\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -457.40\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009130503 |\n",
      "|    clip_fraction        | 0.00327      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.663        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.85         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | 0.000118     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 8.95         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -567.04\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -567.04\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004194609 |\n",
      "|    clip_fraction        | 0.0142      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.79        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00115    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -606.29\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -606.29\n",
      "Reward_Sum: -556.70\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -556.70\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 80            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 202           |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00077293045 |\n",
      "|    clip_fraction        | 0.0236        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.679         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.92          |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.00137      |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 10.7          |\n",
      "-------------------------------------------\n",
      "Reward_Sum: -726.49\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -726.49\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 228          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013407662 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.744        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.98         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 2.72e-05     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 12.1         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -646.11\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -646.11\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 254          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004605631 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.652        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.77         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000303    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 13.2         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -619.65\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -619.65\n",
      "Reward_Sum: -593.67\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -593.67\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 282          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035698737 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.683        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.36         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 10.3         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -576.90\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -576.90\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 80            |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 307           |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034633494 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.749         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 14.8          |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000251     |\n",
      "|    std                  | 0.984         |\n",
      "|    value_loss           | 12.9          |\n",
      "-------------------------------------------\n",
      "Reward_Sum: -529.47\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -529.47\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 332          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022713412 |\n",
      "|    clip_fraction        | 0.00112      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.81         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00066     |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 9.23         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -647.43\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -647.43\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 359          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015512859 |\n",
      "|    clip_fraction        | 0.00166      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.74         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.9          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | 0.000109     |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 8.77         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -616.65\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -616.65\n",
      "Reward_Sum: -586.08\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -586.08\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002444011 |\n",
      "|    clip_fraction        | 0.00142     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.53        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.000676   |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 10.9        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -450.90\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -450.90\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 404          |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007325477 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.18         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000181    |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 11.7         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -506.70\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -506.70\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 426          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013658141 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.75         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.54         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | 4.56e-05     |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 9.41         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -517.93\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -517.93\n",
      "Reward_Sum: -559.27\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -559.27\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 448          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037298414 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.803        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.56         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000736    |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 8.91         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -548.88\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -548.88\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 468          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055889543 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000745    |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 13.7         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -527.55\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -527.55\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 489         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002258802 |\n",
      "|    clip_fraction        | 0.00112     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.18        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.000737   |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 7.54        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -559.59\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -559.59\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 511          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016702645 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000473    |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 8.46         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -541.80\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -541.80\n",
      "Reward_Sum: -668.83\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -668.83\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 533          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023509138 |\n",
      "|    clip_fraction        | 0.0085       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.21         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000483    |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 6.09         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -522.17\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -522.17\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 553         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004809105 |\n",
      "|    clip_fraction        | 0.0237      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.96        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 10.6        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -461.28\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -461.28\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 575          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031777318 |\n",
      "|    clip_fraction        | 0.00562      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.02         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000227    |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 7.21         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -574.40\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -574.40\n",
      "Reward_Sum: -557.81\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -557.81\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 598         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003044526 |\n",
      "|    clip_fraction        | 0.00854     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.000514   |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 7.15        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -564.87\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -564.87\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 618         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005909906 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00294    |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -615.48\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -615.48\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 643         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001095884 |\n",
      "|    clip_fraction        | 0.00156     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.932       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.000191   |\n",
      "|    std                  | 0.973       |\n",
      "|    value_loss           | 6.3         |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -597.67\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -597.67\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 672          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027984087 |\n",
      "|    clip_fraction        | 0.00859      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.65         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000541    |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 5.97         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -646.63\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -646.63\n",
      "Reward_Sum: -545.72\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -545.72\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 84            |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 699           |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087437313 |\n",
      "|    clip_fraction        | 0.00347       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.833         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.869         |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.000147     |\n",
      "|    std                  | 0.972         |\n",
      "|    value_loss           | 7.89          |\n",
      "-------------------------------------------\n",
      "Reward_Sum: -562.50\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -562.50\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 724         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005469959 |\n",
      "|    clip_fraction        | 0.0238      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.93        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.000418   |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 8.37        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -580.26\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -580.26\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 751          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032239985 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.635        |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 5.37         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -655.83\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -655.83\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 780         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006669687 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.87        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 4.49        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -531.26\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -531.26\n",
      "Reward_Sum: -432.05\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -432.05\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 806         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002992882 |\n",
      "|    clip_fraction        | 0.0206      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.85        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.000257   |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 6.68        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -491.06\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -491.06\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 831          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018916095 |\n",
      "|    clip_fraction        | 0.00962      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.14         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -3.91e-05    |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 10           |\n",
      "------------------------------------------\n",
      "Reward_Sum: -352.07\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -352.07\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 857         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005306864 |\n",
      "|    clip_fraction        | 0.019       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 6.63e-05    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 5.72        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -528.66\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -528.66\n",
      "Reward_Sum: -571.50\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -571.50\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 884          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041386336 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.16         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 4.22         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -476.37\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -476.37\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 910         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006120199 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.2         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -512.58\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -512.58\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 935          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045274775 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.23         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.78         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -395.89\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -395.89\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 962         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005657589 |\n",
      "|    clip_fraction        | 0.046       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.61        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.000501   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.01        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -565.80\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -565.80\n",
      "Reward_Sum: -464.41\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -464.41\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 986         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005976571 |\n",
      "|    clip_fraction        | 0.0279      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.783       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.000418    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.25        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -583.79\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -583.79\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 1011        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005012241 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.94        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -3.16e-05   |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 8.13        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -523.30\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -523.30\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 1036        |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008125503 |\n",
      "|    clip_fraction        | 0.0446      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.83        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 7.77        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -570.02\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -570.02\n",
      "Reward_Sum: -455.95\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -455.95\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 1061         |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038831448 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.92         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | 0.00139      |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 5.35         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -498.41\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -498.41\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 1086       |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00267762 |\n",
      "|    clip_fraction        | 0.0381     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.874      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.13       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.00193   |\n",
      "|    std                  | 0.995      |\n",
      "|    value_loss           | 8.93       |\n",
      "----------------------------------------\n",
      "Reward_Sum: -357.17\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -357.17\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 1110        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004762904 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.000489   |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 5.28        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -366.66\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -366.66\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 1139         |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041482914 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.36         |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 4.54         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -352.47\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -352.47\n",
      "Reward_Sum: -170.94\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -170.94\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 1165         |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067648967 |\n",
      "|    clip_fraction        | 0.0766       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | 0.00149      |\n",
      "|    std                  | 0.967        |\n",
      "|    value_loss           | 3.89         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -508.25\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -508.25\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1188        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022291105 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.821       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.0103      |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -434.40\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -434.40\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 1212         |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060484163 |\n",
      "|    clip_fraction        | 0.0858       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.97         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 5.07         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train and save the agent\n",
    "model.learn(total_timesteps=100_000, callback = callback)\n",
    "model.save(\"./model/PPO_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b8cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect te rewards and close the env\n",
    "rewards = np.array(reward_callback.episode_rewards)\n",
    "np.save('rewards.npy', rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad478f",
   "metadata": {},
   "source": [
    "## Evaluate the train process\n",
    "### Image\n",
    "After collecting the rewards, you can draw the rewards during training process here.\n",
    "Apperantly, the reward doesn't converge and the performance is not well. Don't worry,\n",
    "we can continuously train it from the last checkpoint later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f930ccd7507654ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T03:35:29.970382Z",
     "start_time": "2025-08-11T03:35:23.541861Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "绘图功能\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# load the rewards\n",
    "rewards = np.load('rewards.npy')\n",
    "# param：the sliding window for means and std\n",
    "window = 10\n",
    "\n",
    "def plot_rewards(rewards, window=100):\n",
    "    # calculate the means and stds\n",
    "    def moving_avg(x, w):\n",
    "        return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    mean = moving_avg(rewards, window)\n",
    "    std = np.array([\n",
    "        np.std(rewards[max(0, i - window + 1):i + 1])\n",
    "        for i in range(window - 1, len(rewards))\n",
    "    ])\n",
    "\n",
    "    # x-axis\n",
    "    x = np.arange(window - 1, len(rewards))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, mean, label='Mean Reward')\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3, label='±1 Std Dev')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward over Training\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards,window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b0388",
   "metadata": {},
   "source": [
    "### GIF & info\n",
    "You probably wanna see the trained agent's motion in a grphic windows or a gif of \n",
    "the whole moving process. You can create the gif here by play the trained model in\n",
    "our sim env.\n",
    "\n",
    "Moreover, if you wanted to analyse the dynamics of the trained agent, you can save\n",
    "the information, determined in julia files, into a npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d8e6b296ccfe1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-11T03:35:38.095121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia VIV Environment initialized.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python3.1.0\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward_Sum: -282.89\n",
      "done: True\n"
     ]
    }
   ],
   "source": [
    "# create gif after training\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from src.VIV_gym import JuliaEnv\n",
    "from src.gif import create_GIF\n",
    "\n",
    "infos = []\n",
    "\n",
    "# same simulation env while 'render_mode' is 'rgb_array' to create images\n",
    "env = JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, variables = variables, spaces = spaces, verbose=True)\n",
    "\n",
    "# load the trained PPO_model\n",
    "model = PPO.load(\"./model/PPO_model\", env=env)\n",
    "\n",
    "# video frame\n",
    "frames = []\n",
    "\n",
    "# reset the env\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# if 'not done', then continue to perform the simulation operation based on trained model\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# save as gif\n",
    "input_frame = \"images\"\n",
    "output_gif = \"./result/train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)\n",
    "env.close()\n",
    "\n",
    "# save the info\n",
    "np.save(\"info_PPO.npy\", info[\"info\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a087965",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Now you have a npy file which contains all dynamic information, so you can plow a \n",
    "line chart of fluid force, agent applied force and the displacement in y direction.\n",
    "The result is not appropriate, as the train is not 100% converged, you can run the \n",
    "callback this time to continue the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c5881cc034ba9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T10:03:18.150239Z",
     "start_time": "2025-08-06T10:03:17.033327Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "info = np.load(\"info_PPO.npy\", allow_pickle=True)\n",
    "force = [f[\"F\"] for f in info[50:]]\n",
    "y_force = [f[\"fluid_force_y\"] for f in info[50:]]\n",
    "x_force = [f[\"fluid_force_x\"] for f in info[50:]]\n",
    "y_dis = [f[\"y_dis\"] for f in info[50:]]\n",
    "x_dis = [f[\"x_dis\"] for f in info[50:]]\n",
    "\n",
    "x = np.arange(len(y_force))\n",
    "# x2 = np.arange(len(y_dis2))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, force, label=\"y_force\", color=\"red\")\n",
    "plt.plot(x, y_force, label=\"y_fluid\", color=\"blue\")\n",
    "plt.plot(x, y_dis, label=\"y_displacement\", color=\"green\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"force & displacement\")\n",
    "plt.title(\"Force and Displacement in y direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fc109",
   "metadata": {},
   "source": [
    "## Callback\n",
    "You found out that the train doesn't end up with a converged reward. You can\n",
    "start a subsequent process from the checkpoint of 100k steps. Run the other\n",
    "200k steps train now:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a873266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia VIV Environment initialized.\n",
      "Reward_Sum: -455.29\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -455.29\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 91   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 22   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Reward_Sum: -455.06\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -455.06\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077070342 |\n",
      "|    clip_fraction        | 0.0568       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.63         |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.000396    |\n",
      "|    std                  | 0.964        |\n",
      "|    value_loss           | 5.86         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -485.58\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -485.58\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074130045 |\n",
      "|    clip_fraction        | 0.0638       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.807        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.000375    |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 4.46         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -364.72\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -364.72\n",
      "Reward_Sum: -462.27\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -462.27\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072567677 |\n",
      "|    clip_fraction        | 0.1          |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.977        |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00581     |\n",
      "|    std                  | 0.968        |\n",
      "|    value_loss           | 3.2          |\n",
      "------------------------------------------\n",
      "Reward_Sum: -148.70\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -148.70\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015171297 |\n",
      "|    clip_fraction        | 0.0483      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    std                  | 0.949       |\n",
      "|    value_loss           | 6.75        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -287.73\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -287.73\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015982639 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.708       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | 0.00739     |\n",
      "|    std                  | 0.937       |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -274.70\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -274.70\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 178          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035003612 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.586        |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | 0.000376     |\n",
      "|    std                  | 0.92         |\n",
      "|    value_loss           | 2            |\n",
      "------------------------------------------\n",
      "Reward_Sum: -143.61\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -143.61\n",
      "Reward_Sum: -205.85\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -205.85\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 203          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072852336 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.514        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19         |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | 0.00336      |\n",
      "|    std                  | 0.932        |\n",
      "|    value_loss           | 1.75         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -429.24\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -429.24\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007173336 |\n",
      "|    clip_fraction        | 0.0811      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | 0.00123     |\n",
      "|    std                  | 0.926       |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -412.96\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -412.96\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 255          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039177868 |\n",
      "|    clip_fraction        | 0.04         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.703        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.97         |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | 0.00105      |\n",
      "|    std                  | 0.922        |\n",
      "|    value_loss           | 3.93         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -212.69\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -212.69\n",
      "Reward_Sum: -176.45\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -176.45\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 281          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067300117 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.765        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.7          |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | 0.00389      |\n",
      "|    std                  | 0.911        |\n",
      "|    value_loss           | 2.12         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -337.85\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -337.85\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023539487 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.425       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.0133      |\n",
      "|    std                  | 0.942       |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -372.48\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -372.48\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 332          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054291748 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.717        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.37         |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    std                  | 0.933        |\n",
      "|    value_loss           | 1.6          |\n",
      "------------------------------------------\n",
      "Reward_Sum: -175.71\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -175.71\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 359          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052687805 |\n",
      "|    clip_fraction        | 0.0917       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.35         |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.0038      |\n",
      "|    std                  | 0.924        |\n",
      "|    value_loss           | 1.41         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -138.28\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -138.28\n",
      "Reward_Sum: -187.69\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -187.69\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015156526 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00384    |\n",
      "|    std                  | 0.915       |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -105.92\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -105.92\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 408         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013255492 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.717       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.84        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0086     |\n",
      "|    std                  | 0.906       |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -145.22\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -145.22\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 434         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010053952 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.233       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | 0.00519     |\n",
      "|    std                  | 0.879       |\n",
      "|    value_loss           | 0.911       |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -135.07\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -135.07\n",
      "Reward_Sum: -493.72\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -493.72\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 79         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 464        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05864136 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | 0.189      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.27       |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | 0.0095     |\n",
      "|    std                  | 0.887      |\n",
      "|    value_loss           | 0.81       |\n",
      "----------------------------------------\n",
      "Reward_Sum: -336.76\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -336.76\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 487          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061729187 |\n",
      "|    clip_fraction        | 0.0435       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.6          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.84         |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    std                  | 0.885        |\n",
      "|    value_loss           | 4.9          |\n",
      "------------------------------------------\n",
      "Reward_Sum: -252.44\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -252.44\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 511          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069309766 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.556        |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00851     |\n",
      "|    std                  | 0.88         |\n",
      "|    value_loss           | 2.75         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -242.78\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -242.78\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 536         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008400962 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.784       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00677    |\n",
      "|    std                  | 0.865       |\n",
      "|    value_loss           | 1.79        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -289.10\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -289.10\n",
      "Reward_Sum: -143.09\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -143.09\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 561         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008719508 |\n",
      "|    clip_fraction        | 0.0972      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.518       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    std                  | 0.854       |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -110.69\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -110.69\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 586        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01103493 |\n",
      "|    clip_fraction        | 0.0787     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.25      |\n",
      "|    explained_variance   | 0.499      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.32       |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | 0.000669   |\n",
      "|    std                  | 0.839      |\n",
      "|    value_loss           | 2.39       |\n",
      "----------------------------------------\n",
      "Reward_Sum: -74.61\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -74.61\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 613         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009169378 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.627       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    std                  | 0.819       |\n",
      "|    value_loss           | 0.64        |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -127.96\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -127.96\n",
      "Reward_Sum: -80.15\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -80.15\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 640          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135279605 |\n",
      "|    clip_fraction        | 0.0979       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.75         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.145        |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | 0.00316      |\n",
      "|    std                  | 0.813        |\n",
      "|    value_loss           | 0.67         |\n",
      "------------------------------------------\n",
      "Reward_Sum: -118.15\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -118.15\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 665         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013849861 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.204       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | 0.0105      |\n",
      "|    std                  | 0.798       |\n",
      "|    value_loss           | 0.697       |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -94.96\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -94.96\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 691         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032783903 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0958      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | 0.00582     |\n",
      "|    std                  | 0.78        |\n",
      "|    value_loss           | 0.468       |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -72.27\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -72.27\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038133163 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0384      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | 0.00878     |\n",
      "|    std                  | 0.775       |\n",
      "|    value_loss           | 0.312       |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -72.55\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -72.55\n",
      "Reward_Sum: -59.76\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -59.76\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 742         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014594259 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | 0.0048      |\n",
      "|    std                  | 0.76        |\n",
      "|    value_loss           | 0.285       |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -46.67\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -46.67\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 762          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065747127 |\n",
      "|    clip_fraction        | 0.164        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.041        |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | 0.0114       |\n",
      "|    std                  | 0.745        |\n",
      "|    value_loss           | 0.383        |\n",
      "------------------------------------------\n",
      "Reward_Sum: -54.45\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -54.45\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 784        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01672483 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.746      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.035      |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | 0.0121     |\n",
      "|    std                  | 0.733      |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Reward_Sum: -47.91\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -47.91\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 807         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014945602 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.221       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.001       |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -44.49\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -44.49\n",
      "Reward_Sum: -26.94\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -26.94\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 81         |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 827        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03464441 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.726      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.167      |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | 0.00293    |\n",
      "|    std                  | 0.709      |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "Reward_Sum: -26.38\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -26.38\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 847         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010284543 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.158       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | 0.0166      |\n",
      "|    std                  | 0.694       |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -28.68\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -28.68\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 870        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03205096 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.769      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0145     |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | 0.00729    |\n",
      "|    std                  | 0.683      |\n",
      "|    value_loss           | 0.0685     |\n",
      "----------------------------------------\n",
      "Reward_Sum: -29.02\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -29.02\n",
      "Reward_Sum: -21.07\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -21.07\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 891         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053050563 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.032       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | 0.0131      |\n",
      "|    std                  | 0.663       |\n",
      "|    value_loss           | 0.0665      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -22.26\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -22.26\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 911         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017080858 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0209      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.681       |\n",
      "|    value_loss           | 0.0592      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -22.57\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -22.57\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 933         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024219949 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0261      |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.00779     |\n",
      "|    std                  | 0.669       |\n",
      "|    value_loss           | 0.0353      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -19.39\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -19.39\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 955         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018028531 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00338    |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | 0.0226      |\n",
      "|    std                  | 0.662       |\n",
      "|    value_loss           | 0.023       |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -19.80\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -19.80\n",
      "Reward_Sum: -15.40\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -15.40\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 976         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036717128 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000986    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 0.00762     |\n",
      "|    std                  | 0.65        |\n",
      "|    value_loss           | 0.0211      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -20.92\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -20.92\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 997         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076941445 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | 0.0299      |\n",
      "|    std                  | 0.637       |\n",
      "|    value_loss           | 0.0308      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -16.80\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -16.80\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 1019       |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03533589 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.983     |\n",
      "|    explained_variance   | 0.761      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.019      |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | 0.0405     |\n",
      "|    std                  | 0.65       |\n",
      "|    value_loss           | 0.017      |\n",
      "----------------------------------------\n",
      "Reward_Sum: -17.94\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.94\n",
      "Reward_Sum: -18.56\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -18.56\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 1042       |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15700808 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.987     |\n",
      "|    explained_variance   | 0.778      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0225    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | 0.0173     |\n",
      "|    std                  | 0.648      |\n",
      "|    value_loss           | 0.0151     |\n",
      "----------------------------------------\n",
      "Reward_Sum: -15.89\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -15.89\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 84        |\n",
      "|    iterations           | 44        |\n",
      "|    time_elapsed         | 1062      |\n",
      "|    total_timesteps      | 90112     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0280854 |\n",
      "|    clip_fraction        | 0.231     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.982    |\n",
      "|    explained_variance   | 0.752     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0466    |\n",
      "|    n_updates            | 910       |\n",
      "|    policy_gradient_loss | 0.0156    |\n",
      "|    std                  | 0.649     |\n",
      "|    value_loss           | 0.0361    |\n",
      "---------------------------------------\n",
      "Reward_Sum: -17.19\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.19\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 1084       |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07858363 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.988     |\n",
      "|    explained_variance   | 0.632      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0201     |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | 0.0171     |\n",
      "|    std                  | 0.648      |\n",
      "|    value_loss           | 0.0126     |\n",
      "----------------------------------------\n",
      "Reward_Sum: -20.68\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -20.68\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 1106        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067059815 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.988      |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    std                  | 0.65        |\n",
      "|    value_loss           | 0.0153      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -21.09\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -21.09\n",
      "Reward_Sum: -18.49\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -18.49\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 1127       |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06587955 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.988     |\n",
      "|    explained_variance   | 0.653      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0133     |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | 0.0157     |\n",
      "|    std                  | 0.652      |\n",
      "|    value_loss           | 0.0109     |\n",
      "----------------------------------------\n",
      "Reward_Sum: -14.72\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -14.72\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1147        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056348287 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.989      |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0275     |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.0252      |\n",
      "|    std                  | 0.648       |\n",
      "|    value_loss           | 0.0168      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -13.32\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -13.32\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 85        |\n",
      "|    iterations           | 49        |\n",
      "|    time_elapsed         | 1169      |\n",
      "|    total_timesteps      | 100352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0260696 |\n",
      "|    clip_fraction        | 0.273     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.984    |\n",
      "|    explained_variance   | 0.8       |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00289  |\n",
      "|    n_updates            | 960       |\n",
      "|    policy_gradient_loss | 0.0307    |\n",
      "|    std                  | 0.646     |\n",
      "|    value_loss           | 0.0109    |\n",
      "---------------------------------------\n",
      "Reward_Sum: -17.54\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.54\n",
      "Reward_Sum: -18.82\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -18.82\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 1191        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072468504 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.985      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0704      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 0.0304      |\n",
      "|    std                  | 0.646       |\n",
      "|    value_loss           | 0.0136      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -15.68\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -15.68\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 86        |\n",
      "|    iterations           | 51        |\n",
      "|    time_elapsed         | 1211      |\n",
      "|    total_timesteps      | 104448    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0269286 |\n",
      "|    clip_fraction        | 0.249     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.98     |\n",
      "|    explained_variance   | 0.854     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00842   |\n",
      "|    n_updates            | 980       |\n",
      "|    policy_gradient_loss | 0.0168    |\n",
      "|    std                  | 0.643     |\n",
      "|    value_loss           | 0.0137    |\n",
      "---------------------------------------\n",
      "Reward_Sum: -17.88\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.88\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 1232        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016161297 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | 0.00915     |\n",
      "|    std                  | 0.639       |\n",
      "|    value_loss           | 0.00802     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -17.26\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.26\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 1255        |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033860423 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.972      |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0278     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | 0.0503      |\n",
      "|    std                  | 0.642       |\n",
      "|    value_loss           | 0.00983     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -16.44\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -16.44\n",
      "Reward_Sum: -13.86\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -13.86\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 1276       |\n",
      "|    total_timesteps      | 110592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07463685 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.977     |\n",
      "|    explained_variance   | 0.806      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0123     |\n",
      "|    n_updates            | 1010       |\n",
      "|    policy_gradient_loss | 0.0197     |\n",
      "|    std                  | 0.644      |\n",
      "|    value_loss           | 0.00714    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -14.55\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -14.55\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 1296       |\n",
      "|    total_timesteps      | 112640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01480097 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.977     |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00375   |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | 0.017      |\n",
      "|    std                  | 0.641      |\n",
      "|    value_loss           | 0.00905    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -16.19\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -16.19\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 1318        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027749773 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0298      |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | 0.0161      |\n",
      "|    std                  | 0.639       |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -19.90\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -19.90\n",
      "Reward_Sum: -23.70\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -23.70\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 57         |\n",
      "|    time_elapsed         | 1341       |\n",
      "|    total_timesteps      | 116736     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03404388 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.972     |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.101      |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | 0.0194     |\n",
      "|    std                  | 0.64       |\n",
      "|    value_loss           | 0.00586    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -20.57\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -20.57\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 1361        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061688915 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.974      |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00457    |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | 0.0266      |\n",
      "|    std                  | 0.641       |\n",
      "|    value_loss           | 0.0208      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -19.38\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -19.38\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 1382       |\n",
      "|    total_timesteps      | 120832     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07538065 |\n",
      "|    clip_fraction        | 0.433      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.985     |\n",
      "|    explained_variance   | 0.736      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.038      |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | 0.0773     |\n",
      "|    std                  | 0.655      |\n",
      "|    value_loss           | 0.0141     |\n",
      "----------------------------------------\n",
      "Reward_Sum: -18.24\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -18.24\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 1407        |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019750372 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0847      |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | 0.0247      |\n",
      "|    std                  | 0.658       |\n",
      "|    value_loss           | 0.0264      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -20.17\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -20.17\n",
      "Reward_Sum: -16.76\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -16.76\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 1434        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023692852 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.997      |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0243      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | 0.0121      |\n",
      "|    std                  | 0.655       |\n",
      "|    value_loss           | 0.00826     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -18.91\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -18.91\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 1455       |\n",
      "|    total_timesteps      | 126976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02031134 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.994     |\n",
      "|    explained_variance   | 0.789      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0068    |\n",
      "|    n_updates            | 1090       |\n",
      "|    policy_gradient_loss | 0.0176     |\n",
      "|    std                  | 0.655      |\n",
      "|    value_loss           | 0.00956    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -17.83\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.83\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 1476        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027932335 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0421      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | 0.0207      |\n",
      "|    std                  | 0.658       |\n",
      "|    value_loss           | 0.00777     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -18.38\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -18.38\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 1499       |\n",
      "|    total_timesteps      | 131072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01602804 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.997     |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00259    |\n",
      "|    n_updates            | 1110       |\n",
      "|    policy_gradient_loss | 0.0238     |\n",
      "|    std                  | 0.653      |\n",
      "|    value_loss           | 0.00784    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -17.68\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.68\n",
      "Reward_Sum: -15.38\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -15.38\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 1520        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026661824 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00614     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | 0.0313      |\n",
      "|    std                  | 0.658       |\n",
      "|    value_loss           | 0.0119      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -17.00\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.00\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 1545        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041831937 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00496    |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | 0.0156      |\n",
      "|    std                  | 0.657       |\n",
      "|    value_loss           | 0.00763     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -14.54\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -14.54\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 1570        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030058397 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0097      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | 0.0278      |\n",
      "|    std                  | 0.666       |\n",
      "|    value_loss           | 0.0116      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -14.75\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -14.75\n",
      "Reward_Sum: -15.10\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -15.10\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 1595        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028325547 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | 0.0183      |\n",
      "|    std                  | 0.663       |\n",
      "|    value_loss           | 0.0055      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -23.67\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -23.67\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 1619       |\n",
      "|    total_timesteps      | 141312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10835841 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.758      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0384     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | 0.0353     |\n",
      "|    std                  | 0.663      |\n",
      "|    value_loss           | 0.00832    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -19.27\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -19.27\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1644        |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034569602 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00938     |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | 0.0504      |\n",
      "|    std                  | 0.683       |\n",
      "|    value_loss           | 0.0237      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -15.27\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -15.27\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 1671        |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100244224 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | 0.0229      |\n",
      "|    std                  | 0.689       |\n",
      "|    value_loss           | 0.0257      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -15.04\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -15.04\n",
      "Reward_Sum: -17.52\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.52\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 1697       |\n",
      "|    total_timesteps      | 147456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04880078 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.628      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.058      |\n",
      "|    n_updates            | 1190       |\n",
      "|    policy_gradient_loss | 0.0246     |\n",
      "|    std                  | 0.68       |\n",
      "|    value_loss           | 0.00645    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -17.01\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -17.01\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 1722        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101229236 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | 0.0218      |\n",
      "|    std                  | 0.678       |\n",
      "|    value_loss           | 0.0141      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -11.18\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -11.18\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 1749       |\n",
      "|    total_timesteps      | 151552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06265253 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.825      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0113    |\n",
      "|    n_updates            | 1210       |\n",
      "|    policy_gradient_loss | 0.0133     |\n",
      "|    std                  | 0.676      |\n",
      "|    value_loss           | 0.00485    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -13.23\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -13.23\n",
      "Reward_Sum: -15.01\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -15.01\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 1776       |\n",
      "|    total_timesteps      | 153600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05143533 |\n",
      "|    clip_fraction        | 0.246      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.819      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0116     |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | 0.0186     |\n",
      "|    std                  | 0.673      |\n",
      "|    value_loss           | 0.00406    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -11.69\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -11.69\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 86        |\n",
      "|    iterations           | 76        |\n",
      "|    time_elapsed         | 1801      |\n",
      "|    total_timesteps      | 155648    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0165574 |\n",
      "|    clip_fraction        | 0.329     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.02     |\n",
      "|    explained_variance   | 0.824     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00384  |\n",
      "|    n_updates            | 1230      |\n",
      "|    policy_gradient_loss | 0.03      |\n",
      "|    std                  | 0.672     |\n",
      "|    value_loss           | 0.00854   |\n",
      "---------------------------------------\n",
      "Reward_Sum: -11.26\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -11.26\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 1826        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022789408 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00763     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | 0.0201      |\n",
      "|    std                  | 0.667       |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -12.09\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -12.09\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 1853       |\n",
      "|    total_timesteps      | 159744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43449157 |\n",
      "|    clip_fraction        | 0.298      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.805      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.187      |\n",
      "|    n_updates            | 1250       |\n",
      "|    policy_gradient_loss | 0.042      |\n",
      "|    std                  | 0.671      |\n",
      "|    value_loss           | 0.00683    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -24.79\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -24.79\n",
      "Reward_Sum: -22.29\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -22.29\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 79           |\n",
      "|    time_elapsed         | 1878         |\n",
      "|    total_timesteps      | 161792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090302965 |\n",
      "|    clip_fraction        | 0.22         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0123      |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | 0.00532      |\n",
      "|    std                  | 0.671        |\n",
      "|    value_loss           | 0.0125       |\n",
      "------------------------------------------\n",
      "Reward_Sum: -20.93\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -20.93\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1903        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025772601 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0134      |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | 0.0156      |\n",
      "|    std                  | 0.668       |\n",
      "|    value_loss           | 0.0169      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -16.65\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -16.65\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 1930        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025188752 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0449     |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | 0.0125      |\n",
      "|    std                  | 0.668       |\n",
      "|    value_loss           | 0.0058      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -18.46\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -18.46\n",
      "Reward_Sum: -15.66\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -15.66\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1958        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039353505 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | 0.0276      |\n",
      "|    std                  | 0.669       |\n",
      "|    value_loss           | 0.00629     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -16.43\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -16.43\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 1983       |\n",
      "|    total_timesteps      | 169984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07473284 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.873      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0648     |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | 0.0377     |\n",
      "|    std                  | 0.672      |\n",
      "|    value_loss           | 0.00832    |\n",
      "----------------------------------------\n",
      "Reward_Sum: -19.10\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -19.10\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 84         |\n",
      "|    time_elapsed         | 2008       |\n",
      "|    total_timesteps      | 172032     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04350289 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00411   |\n",
      "|    n_updates            | 1310       |\n",
      "|    policy_gradient_loss | 0.0168     |\n",
      "|    std                  | 0.671      |\n",
      "|    value_loss           | 0.007      |\n",
      "----------------------------------------\n",
      "Reward_Sum: -20.19\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -20.19\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 2034        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020689499 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.028       |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | 0.0201      |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 0.00723     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -24.93\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -24.93\n",
      "Reward_Sum: -28.16\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -28.16\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 2059       |\n",
      "|    total_timesteps      | 176128     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03042605 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.76       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0673     |\n",
      "|    n_updates            | 1330       |\n",
      "|    policy_gradient_loss | 0.0346     |\n",
      "|    std                  | 0.676      |\n",
      "|    value_loss           | 0.0286     |\n",
      "----------------------------------------\n",
      "Reward_Sum: -23.94\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -23.94\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 2086        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014347801 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.55        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0564      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | 0.0237      |\n",
      "|    std                  | 0.68        |\n",
      "|    value_loss           | 0.0355      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -21.84\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -21.84\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 88         |\n",
      "|    time_elapsed         | 2112       |\n",
      "|    total_timesteps      | 180224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05912754 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.592      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0142     |\n",
      "|    n_updates            | 1350       |\n",
      "|    policy_gradient_loss | 0.0214     |\n",
      "|    std                  | 0.685      |\n",
      "|    value_loss           | 0.0171     |\n",
      "----------------------------------------\n",
      "Reward_Sum: -22.25\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -22.25\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 2139        |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036957994 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0317      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | 0.0357      |\n",
      "|    std                  | 0.694       |\n",
      "|    value_loss           | 0.0176      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -23.86\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -23.86\n",
      "Reward_Sum: -22.90\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -22.90\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 2164        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021744307 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | 0.0417      |\n",
      "|    std                  | 0.695       |\n",
      "|    value_loss           | 0.0235      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -16.42\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -16.42\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 85        |\n",
      "|    iterations           | 91        |\n",
      "|    time_elapsed         | 2189      |\n",
      "|    total_timesteps      | 186368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0226903 |\n",
      "|    clip_fraction        | 0.279     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.06     |\n",
      "|    explained_variance   | 0.52      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.021    |\n",
      "|    n_updates            | 1380      |\n",
      "|    policy_gradient_loss | 0.0381    |\n",
      "|    std                  | 0.699     |\n",
      "|    value_loss           | 0.0213    |\n",
      "---------------------------------------\n",
      "Reward_Sum: -20.17\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -20.17\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 2214        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011582989 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00304     |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | 0.0214      |\n",
      "|    std                  | 0.697       |\n",
      "|    value_loss           | 0.00877     |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -20.41\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -20.41\n",
      "Reward_Sum: -24.45\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -24.45\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 2239       |\n",
      "|    total_timesteps      | 190464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10862989 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.06      |\n",
      "|    explained_variance   | 0.603      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0352    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | 0.0369     |\n",
      "|    std                  | 0.701      |\n",
      "|    value_loss           | 0.0101     |\n",
      "----------------------------------------\n",
      "Reward_Sum: -25.74\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -25.74\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 85        |\n",
      "|    iterations           | 94        |\n",
      "|    time_elapsed         | 2262      |\n",
      "|    total_timesteps      | 192512    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0826732 |\n",
      "|    clip_fraction        | 0.265     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.07     |\n",
      "|    explained_variance   | 0.474     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00587   |\n",
      "|    n_updates            | 1410      |\n",
      "|    policy_gradient_loss | 0.0263    |\n",
      "|    std                  | 0.706     |\n",
      "|    value_loss           | 0.024     |\n",
      "---------------------------------------\n",
      "Reward_Sum: -21.25\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -21.25\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 2287        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038305335 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0372      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | 0.0211      |\n",
      "|    std                  | 0.719       |\n",
      "|    value_loss           | 0.0181      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -23.33\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -23.33\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 2312        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034827102 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0636      |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | 0.00698     |\n",
      "|    std                  | 0.72        |\n",
      "|    value_loss           | 0.0171      |\n",
      "-----------------------------------------\n",
      "Reward_Sum: -20.74\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -20.74\n",
      "Reward_Sum: -21.60\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -21.60\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 2335       |\n",
      "|    total_timesteps      | 198656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02589488 |\n",
      "|    clip_fraction        | 0.276      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.651      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0348     |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | 0.015      |\n",
      "|    std                  | 0.724      |\n",
      "|    value_loss           | 0.0133     |\n",
      "----------------------------------------\n",
      "Reward_Sum: -13.75\n",
      "done: True\n",
      "Episode finished after 1599 steps\n",
      "Episode reward: -13.75\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 98         |\n",
      "|    time_elapsed         | 2359       |\n",
      "|    total_timesteps      | 200704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03936084 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.659      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.035     |\n",
      "|    n_updates            | 1450       |\n",
      "|    policy_gradient_loss | 0.0313     |\n",
      "|    std                  | 0.723      |\n",
      "|    value_loss           | 0.04       |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "加载checkpoint并继续训练\n",
    "\n",
    "\"\"\"\n",
    "env = DummyVecEnv([lambda: JuliaEnv(render_mode=None, env = VIVEnv, max_episode_steps=2000, statics = statics, \n",
    "                                    variables = variables, spaces = spaces, verbose=1)])\n",
    "\n",
    "reward_callback = RewardLoggerCallback()\n",
    "callback = CallbackList([checkpoint_callback, reward_callback])\n",
    "\n",
    "model = PPO.load(\"./checkpoints/ppo_model_100000_steps\", env=env, device='cpu')\n",
    "model.learn(total_timesteps=200_000, callback = callback)\n",
    "rewards_ex = np.array(reward_callback.episode_rewards)\n",
    "rewards = np.load('rewards.npy')\n",
    "rewards = np.concatenate([rewards, rewards_ex])\n",
    "np.save('rewards.npy', rewards)\n",
    "model.save(\"./model/PPO_model_100k-300k\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a769d4f",
   "metadata": {},
   "source": [
    "Plot the means and stds of the rewards now. You can find it is nearly converged.\n",
    "Apparently, the agent learned a policy to execute the force to counteract the lift \n",
    "force, keeps the agent around y=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Reward Image\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# load the rewards\n",
    "rewards = np.load('rewards.npy')\n",
    "# param：the sliding window for means and std\n",
    "window = 10\n",
    "\n",
    "def plot_rewards(rewards, window=100):\n",
    "    episode = np.arange(len(rewards))\n",
    "\n",
    "    # calculate the means and stds\n",
    "    def moving_avg(x, w):\n",
    "        return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "    mean = moving_avg(rewards, window)\n",
    "    std = np.array([\n",
    "        np.std(rewards[max(0, i - window + 1):i + 1])\n",
    "        for i in range(window - 1, len(rewards))\n",
    "    ])\n",
    "\n",
    "    # x-axis\n",
    "    x = np.arange(window - 1, len(rewards))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, mean, label='Mean Reward')\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3, label='±1 Std Dev')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward over 250k Steps Training\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rewards(rewards,window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5201e1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia VIV Environment initialized.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Reward_Sum: -14.97\n",
      "done: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "GIF\n",
    "\n",
    "\"\"\"\n",
    "# create gif after training\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from src.VIV_gym import JuliaEnv\n",
    "from src.gif import create_GIF\n",
    "\n",
    "infos = []\n",
    "\n",
    "# same simulation env while 'render_mode' is 'rgb_array' to create images\n",
    "env = JuliaEnv(render_mode=\"rgb_array\", env = VIVEnv, max_episode_steps=2000, statics = statics, variables = variables, spaces = spaces, verbose=True)\n",
    "\n",
    "# load the trained PPO_model\n",
    "model = PPO.load(\"./model/PPO_model_100k-200k\", env=env)\n",
    "\n",
    "# video frame\n",
    "frames = []\n",
    "\n",
    "# reset the env\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# if 'not done', then continue to perform the simulation operation based on trained model\n",
    "while not done and not truncated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "# save as gif\n",
    "input_frame = \"images\"\n",
    "output_gif = \"./result/train_policy_demo.gif\"\n",
    "create_GIF(input_frame, output_gif)\n",
    "env.close()\n",
    "\n",
    "# save the info\n",
    "np.save(\"info_PPO.npy\", info[\"info\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329182e",
   "metadata": {},
   "source": [
    "You will find that the agent is completely trained, and the y-displacement are\n",
    "nearly 0 all time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1e75131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "info = np.load(\"info_PPO.npy\", allow_pickle=True)\n",
    "force = [f[\"F\"] for f in info[50:]]\n",
    "y_force = [f[\"fluid_force_y\"] for f in info[50:]]\n",
    "x_force = [f[\"fluid_force_x\"] for f in info[50:]]\n",
    "y_dis = [f[\"y_dis\"] for f in info[50:]]\n",
    "x_dis = [f[\"x_dis\"] for f in info[50:]]\n",
    "\n",
    "x = np.arange(len(y_force))\n",
    "# x2 = np.arange(len(y_dis2))\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, force, label=\"y_force\", color=\"red\")\n",
    "plt.plot(x, y_force, label=\"y_fluid\", color=\"blue\")\n",
    "plt.plot(x, y_dis, label=\"y_displacement\", color=\"green\")\n",
    "\n",
    "# 图例、标签、标题\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"force & displacement\")\n",
    "plt.title(\"Force and Displacement in y direction\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
